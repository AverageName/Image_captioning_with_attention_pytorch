{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import os\n",
    "import operator\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is nancy'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_caption(caption):\n",
    "    caption = ''.join([char for char in caption if not char in punctuation]).lower()\n",
    "    return caption\n",
    "prepare_caption('My name is Nancy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Flickr30k(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir, csv_file, transform=None, topk=5000):\n",
    "        self.df = pd.read_csv(os.path.join(root_dir, csv_file), delimiter='|')\n",
    "        self.df.iloc[19999][' comment_number'] = ' 4'\n",
    "        self.df.iloc[19999][' comment'] = ' A dog runs across the grass .'\n",
    "        self.captions = {}\n",
    "        self.vocab = Counter()\n",
    "        for idx, row in self.df.iterrows():\n",
    "            img_path = row['image_name']\n",
    "            caption = prepare_caption(row[' comment'])\n",
    "            if img_path not in self.captions:\n",
    "                self.captions.update({img_path: [caption]})\n",
    "            else:\n",
    "                self.captions[img_path].append(caption)\n",
    "                \n",
    "            for word in caption.split():\n",
    "                self.vocab[word] += 1\n",
    "                \n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.topk = topk\n",
    "        self.word2index = {word: index for index, (word, count) in enumerate(sorted(self.vocab.items(), key=operator.itemgetter(1), reverse=True)[:topk])}\n",
    "        self.index2word = {index: word for index, (word, count) in enumerate(sorted(self.vocab.items(), key=operator.itemgetter(1), reverse=True)[:topk])}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "    def __getitem__(self, x):\n",
    "        img_name = self.df.iloc[x * 5, 0]\n",
    "        img = Image.open(os.path.join(self.root_dir, 'flickr30k_images', img_name))\n",
    "        caption = sorted(self.captions[img_name], key=len)[-1]\n",
    "        caption_encoded = []\n",
    "        for word in caption.split():\n",
    "            if word not in self.word2index:\n",
    "                caption_encoded.append(self.topk)\n",
    "            else:\n",
    "                caption_encoded.append(self.word2index[word])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, caption, caption_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(tensors):\n",
    "    seq_len = max([tensor.shape[0] for tensor in tensors])\n",
    "    for i in range(len(tensors)):\n",
    "        if tensors[i].shape[0] < seq_len:\n",
    "            tensors[i] = torch.cat([tensors[i], torch.zeros(seq_len - tensors[i].shape[0])], dim=-1)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = [example[0] for example in batch]\n",
    "    captions = [example[1] for example in batch]\n",
    "    captions_encoded = [torch.Tensor(example[2]) for example in batch]\n",
    "    \n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    captions_encoded = pad_seq(captions_encoded)\n",
    "    captions_encoded = torch.stack(captions_encoded, dim=0)\n",
    "    \n",
    "    return imgs, captions, captions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Flickr30k('/mnt/c/Users/MAX/Downloads/flickr30k_images', 'results.csv', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=2, drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1, 2, 3), device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "1 torch.Size([2, 3, 224, 224]) torch.Size([2, 17])\n",
      "2 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "3 torch.Size([2, 3, 224, 224]) torch.Size([2, 15])\n",
      "4 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "5 torch.Size([2, 3, 224, 224]) torch.Size([2, 22])\n",
      "6 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "7 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "8 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "9 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "10 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "11 torch.Size([2, 3, 224, 224]) torch.Size([2, 26])\n",
      "12 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "13 torch.Size([2, 3, 224, 224]) torch.Size([2, 28])\n",
      "14 torch.Size([2, 3, 224, 224]) torch.Size([2, 14])\n",
      "15 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "16 torch.Size([2, 3, 224, 224]) torch.Size([2, 14])\n",
      "17 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "18 torch.Size([2, 3, 224, 224]) torch.Size([2, 37])\n",
      "19 torch.Size([2, 3, 224, 224]) torch.Size([2, 21])\n",
      "20 torch.Size([2, 3, 224, 224]) torch.Size([2, 35])\n",
      "21 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "22 torch.Size([2, 3, 224, 224]) torch.Size([2, 22])\n",
      "23 torch.Size([2, 3, 224, 224]) torch.Size([2, 14])\n",
      "24 torch.Size([2, 3, 224, 224]) torch.Size([2, 21])\n",
      "25 torch.Size([2, 3, 224, 224]) torch.Size([2, 29])\n",
      "26 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "27 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "28 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "29 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "30 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "31 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "32 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "33 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "34 torch.Size([2, 3, 224, 224]) torch.Size([2, 27])\n",
      "35 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "36 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "37 torch.Size([2, 3, 224, 224]) torch.Size([2, 15])\n",
      "38 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "39 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "40 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "41 torch.Size([2, 3, 224, 224]) torch.Size([2, 11])\n",
      "42 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "43 torch.Size([2, 3, 224, 224]) torch.Size([2, 14])\n",
      "44 torch.Size([2, 3, 224, 224]) torch.Size([2, 49])\n",
      "45 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "46 torch.Size([2, 3, 224, 224]) torch.Size([2, 24])\n",
      "47 torch.Size([2, 3, 224, 224]) torch.Size([2, 31])\n",
      "48 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "49 torch.Size([2, 3, 224, 224]) torch.Size([2, 37])\n",
      "50 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "51 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "52 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "53 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "54 torch.Size([2, 3, 224, 224]) torch.Size([2, 23])\n",
      "55 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "56 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "57 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "58 torch.Size([2, 3, 224, 224]) torch.Size([2, 22])\n",
      "59 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "60 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "61 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "62 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "63 torch.Size([2, 3, 224, 224]) torch.Size([2, 31])\n",
      "64 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "65 torch.Size([2, 3, 224, 224]) torch.Size([2, 24])\n",
      "66 torch.Size([2, 3, 224, 224]) torch.Size([2, 41])\n",
      "67 torch.Size([2, 3, 224, 224]) torch.Size([2, 35])\n",
      "68 torch.Size([2, 3, 224, 224]) torch.Size([2, 13])\n",
      "69 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "70 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "71 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "72 torch.Size([2, 3, 224, 224]) torch.Size([2, 27])\n",
      "73 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "74 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "75 torch.Size([2, 3, 224, 224]) torch.Size([2, 29])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0ee7edd4ab35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-afa33c340448>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_encoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1802\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(data_loader):\n",
    "    print(idx, data[0].shape, data[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "    \n",
    "        self.model = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        \n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, features_dim, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(features_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, features, hidden):\n",
    "        features = self.linear1(features)\n",
    "        hidden = hidden[0]\n",
    "        \n",
    "        hidden = self.linear2(hidden)\n",
    "        #print(hidden.shape)\n",
    "        #rint(features.shape)\n",
    "        \n",
    "        score = torch.nn.functional.tanh(hidden.permute(1, 0, 2) + features)\n",
    "        \n",
    "        attention_weights = torch.nn.functional.softmax(self.linear3(score), dim=1)\n",
    "        \n",
    "        attention_vectors = attention_weights * score\n",
    "        attention_vectors = torch.mean(attention_vectors, dim=1)\n",
    "        \n",
    "        return attention_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, embedding_dim, num_layers, vocab_len):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_len, embedding_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim + embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_len)\n",
    "        self.attention = Attention(embedding_dim, hidden_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        if torch.cuda.is_available():\n",
    "            hidden = (torch.zeros((batch_size, self.num_layers, self.hidden_dim), device=torch.device('cuda')),\n",
    "                     torch.zeros((batch_size, self.num_layers, self.hidden_dim), device=torch.device('cuda'))\n",
    "                     )\n",
    "        else:\n",
    "            hidden = (torch.zeros((self.num_layers, batch_size, self.hidden_dim), device=torch.device('cpu')),\n",
    "                     torch.zeros((self.num_layers, batch_size, self.hidden_dim), device=torch.device('cpu'))\n",
    "                     )\n",
    "            \n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x, enc_embed, hidden):\n",
    "        embeds = self.embeddings(x)\n",
    "        embeds = torch.unsqueeze(embeds, 1)\n",
    "        attention_vectors = self.attention(enc_embed, hidden)\n",
    "        attention_vectors = torch.unsqueeze(attention_vectors, 1)\n",
    "        #print(attention_vectors.shape)\n",
    "        x = torch.cat([embeds, attention_vectors], dim=-1)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.linear(x)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(decoder.parameters())\n",
    "    for idx, data in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = data[0].float()\n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        enc_embeds = encoder(inputs)\n",
    "        enc_embeds = enc_embeds.view(batch_size, encoder.model.inplanes, -1)\n",
    "        labels = data[2].long()\n",
    "        hidden = decoder.init_hidden(batch_size)\n",
    "        #print(hidden[0].shape)\n",
    "        outputs = []\n",
    "        for i in range(labels.shape[1]):\n",
    "            output, hidden = decoder(labels[:, i], enc_embeds, hidden)\n",
    "            outputs.append(output)\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        outputs = outputs.view(-1, dataset.topk + 1)\n",
    "        labels = labels.view(-1)\n",
    "        #print(outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.5243, grad_fn=<NllLossBackward>)\n",
      "tensor(8.5587, grad_fn=<NllLossBackward>)\n",
      "tensor(8.5411, grad_fn=<NllLossBackward>)\n",
      "tensor(8.5216, grad_fn=<NllLossBackward>)\n",
      "tensor(8.5077, grad_fn=<NllLossBackward>)\n",
      "tensor(8.4826, grad_fn=<NllLossBackward>)\n",
      "tensor(8.5056, grad_fn=<NllLossBackward>)\n",
      "tensor(8.4636, grad_fn=<NllLossBackward>)\n",
      "tensor(8.4529, grad_fn=<NllLossBackward>)\n",
      "tensor(8.4848, grad_fn=<NllLossBackward>)\n",
      "tensor(8.4089, grad_fn=<NllLossBackward>)\n",
      "tensor(8.3534, grad_fn=<NllLossBackward>)\n",
      "tensor(8.4570, grad_fn=<NllLossBackward>)\n",
      "tensor(8.3427, grad_fn=<NllLossBackward>)\n",
      "tensor(8.4651, grad_fn=<NllLossBackward>)\n",
      "tensor(8.3772, grad_fn=<NllLossBackward>)\n",
      "tensor(8.3399, grad_fn=<NllLossBackward>)\n",
      "tensor(8.4105, grad_fn=<NllLossBackward>)\n",
      "tensor(8.3154, grad_fn=<NllLossBackward>)\n",
      "tensor(8.3249, grad_fn=<NllLossBackward>)\n",
      "tensor(8.2179, grad_fn=<NllLossBackward>)\n",
      "tensor(8.3101, grad_fn=<NllLossBackward>)\n",
      "tensor(8.3252, grad_fn=<NllLossBackward>)\n",
      "tensor(8.3901, grad_fn=<NllLossBackward>)\n",
      "tensor(8.1893, grad_fn=<NllLossBackward>)\n",
      "tensor(8.1892, grad_fn=<NllLossBackward>)\n",
      "tensor(8.3757, grad_fn=<NllLossBackward>)\n",
      "tensor(8.3386, grad_fn=<NllLossBackward>)\n",
      "tensor(8.2220, grad_fn=<NllLossBackward>)\n",
      "tensor(8.2163, grad_fn=<NllLossBackward>)\n",
      "tensor(8.2300, grad_fn=<NllLossBackward>)\n",
      "tensor(8.1872, grad_fn=<NllLossBackward>)\n",
      "tensor(8.2222, grad_fn=<NllLossBackward>)\n",
      "tensor(8.1134, grad_fn=<NllLossBackward>)\n",
      "tensor(8.0669, grad_fn=<NllLossBackward>)\n",
      "tensor(8.1259, grad_fn=<NllLossBackward>)\n",
      "tensor(7.8829, grad_fn=<NllLossBackward>)\n",
      "tensor(8.2620, grad_fn=<NllLossBackward>)\n",
      "tensor(8.2719, grad_fn=<NllLossBackward>)\n",
      "tensor(8.1476, grad_fn=<NllLossBackward>)\n",
      "tensor(8.1440, grad_fn=<NllLossBackward>)\n",
      "tensor(8.1449, grad_fn=<NllLossBackward>)\n",
      "tensor(8.1407, grad_fn=<NllLossBackward>)\n",
      "tensor(8.2524, grad_fn=<NllLossBackward>)\n",
      "tensor(7.6089, grad_fn=<NllLossBackward>)\n",
      "tensor(8.2150, grad_fn=<NllLossBackward>)\n",
      "tensor(8.1691, grad_fn=<NllLossBackward>)\n",
      "tensor(7.6921, grad_fn=<NllLossBackward>)\n",
      "tensor(7.7727, grad_fn=<NllLossBackward>)\n",
      "tensor(7.4653, grad_fn=<NllLossBackward>)\n",
      "tensor(8.1007, grad_fn=<NllLossBackward>)\n",
      "tensor(7.9833, grad_fn=<NllLossBackward>)\n",
      "tensor(8.2908, grad_fn=<NllLossBackward>)\n",
      "tensor(8.0983, grad_fn=<NllLossBackward>)\n",
      "tensor(7.6290, grad_fn=<NllLossBackward>)\n",
      "tensor(8.0228, grad_fn=<NllLossBackward>)\n",
      "tensor(7.7568, grad_fn=<NllLossBackward>)\n",
      "tensor(7.2866, grad_fn=<NllLossBackward>)\n",
      "tensor(7.3932, grad_fn=<NllLossBackward>)\n",
      "tensor(7.5792, grad_fn=<NllLossBackward>)\n",
      "tensor(7.6644, grad_fn=<NllLossBackward>)\n",
      "tensor(7.3382, grad_fn=<NllLossBackward>)\n",
      "tensor(7.3481, grad_fn=<NllLossBackward>)\n",
      "tensor(7.0165, grad_fn=<NllLossBackward>)\n",
      "tensor(7.6966, grad_fn=<NllLossBackward>)\n",
      "tensor(7.8998, grad_fn=<NllLossBackward>)\n",
      "tensor(7.3460, grad_fn=<NllLossBackward>)\n",
      "tensor(7.3324, grad_fn=<NllLossBackward>)\n",
      "tensor(7.2324, grad_fn=<NllLossBackward>)\n",
      "tensor(7.3389, grad_fn=<NllLossBackward>)\n",
      "tensor(7.0455, grad_fn=<NllLossBackward>)\n",
      "tensor(7.5714, grad_fn=<NllLossBackward>)\n",
      "tensor(7.3427, grad_fn=<NllLossBackward>)\n",
      "tensor(7.9613, grad_fn=<NllLossBackward>)\n",
      "tensor(7.3382, grad_fn=<NllLossBackward>)\n",
      "tensor(6.9074, grad_fn=<NllLossBackward>)\n",
      "tensor(6.8695, grad_fn=<NllLossBackward>)\n",
      "tensor(7.2560, grad_fn=<NllLossBackward>)\n",
      "tensor(7.9055, grad_fn=<NllLossBackward>)\n",
      "tensor(7.2222, grad_fn=<NllLossBackward>)\n",
      "tensor(7.6556, grad_fn=<NllLossBackward>)\n",
      "tensor(6.9819, grad_fn=<NllLossBackward>)\n",
      "tensor(7.6571, grad_fn=<NllLossBackward>)\n",
      "tensor(7.2960, grad_fn=<NllLossBackward>)\n",
      "tensor(7.2708, grad_fn=<NllLossBackward>)\n",
      "tensor(7.5299, grad_fn=<NllLossBackward>)\n",
      "tensor(7.1382, grad_fn=<NllLossBackward>)\n",
      "tensor(7.2682, grad_fn=<NllLossBackward>)\n",
      "tensor(6.6945, grad_fn=<NllLossBackward>)\n",
      "tensor(6.9991, grad_fn=<NllLossBackward>)\n",
      "tensor(7.0652, grad_fn=<NllLossBackward>)\n",
      "tensor(6.8367, grad_fn=<NllLossBackward>)\n",
      "tensor(6.5237, grad_fn=<NllLossBackward>)\n",
      "tensor(6.8287, grad_fn=<NllLossBackward>)\n",
      "tensor(6.5958, grad_fn=<NllLossBackward>)\n",
      "tensor(6.5835, grad_fn=<NllLossBackward>)\n",
      "tensor(6.3373, grad_fn=<NllLossBackward>)\n",
      "tensor(6.7248, grad_fn=<NllLossBackward>)\n",
      "tensor(7.1973, grad_fn=<NllLossBackward>)\n",
      "tensor(6.5714, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1654, grad_fn=<NllLossBackward>)\n",
      "tensor(6.5612, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1651, grad_fn=<NllLossBackward>)\n",
      "tensor(6.7866, grad_fn=<NllLossBackward>)\n",
      "tensor(6.8580, grad_fn=<NllLossBackward>)\n",
      "tensor(5.8544, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1714, grad_fn=<NllLossBackward>)\n",
      "tensor(6.0780, grad_fn=<NllLossBackward>)\n",
      "tensor(5.9809, grad_fn=<NllLossBackward>)\n",
      "tensor(6.4551, grad_fn=<NllLossBackward>)\n",
      "tensor(6.5921, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1862, grad_fn=<NllLossBackward>)\n",
      "tensor(7.2892, grad_fn=<NllLossBackward>)\n",
      "tensor(6.3696, grad_fn=<NllLossBackward>)\n",
      "tensor(6.5659, grad_fn=<NllLossBackward>)\n",
      "tensor(6.3903, grad_fn=<NllLossBackward>)\n",
      "tensor(6.7892, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7314, grad_fn=<NllLossBackward>)\n",
      "tensor(6.3476, grad_fn=<NllLossBackward>)\n",
      "tensor(6.7113, grad_fn=<NllLossBackward>)\n",
      "tensor(6.0998, grad_fn=<NllLossBackward>)\n",
      "tensor(6.7941, grad_fn=<NllLossBackward>)\n",
      "tensor(6.6570, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4360, grad_fn=<NllLossBackward>)\n",
      "tensor(6.2436, grad_fn=<NllLossBackward>)\n",
      "tensor(6.3202, grad_fn=<NllLossBackward>)\n",
      "tensor(6.2018, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9744, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5258, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6528, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0422, grad_fn=<NllLossBackward>)\n",
      "tensor(6.0894, grad_fn=<NllLossBackward>)\n",
      "tensor(6.7355, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1105, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7885, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2224, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7376, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4678, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5627, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6927, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5759, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5855, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6796, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7371, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5164, grad_fn=<NllLossBackward>)\n",
      "tensor(6.2487, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3850, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7939, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4420, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7529, grad_fn=<NllLossBackward>)\n",
      "tensor(6.4044, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7934, grad_fn=<NllLossBackward>)\n",
      "tensor(6.2842, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5363, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5546, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0254, grad_fn=<NllLossBackward>)\n",
      "tensor(6.4629, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3756, grad_fn=<NllLossBackward>)\n",
      "tensor(5.8173, grad_fn=<NllLossBackward>)\n",
      "tensor(6.2067, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4770, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2660, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3090, grad_fn=<NllLossBackward>)\n",
      "tensor(6.2197, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7436, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6100, grad_fn=<NllLossBackward>)\n",
      "tensor(5.9283, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2848, grad_fn=<NllLossBackward>)\n",
      "tensor(5.9137, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3826, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5498, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0672, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9329, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3813, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6401, grad_fn=<NllLossBackward>)\n",
      "tensor(5.8747, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3057, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4401, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9574, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5902, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5414, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5773, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0172, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7005, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7506, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6479, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2137, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1476, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4520, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7337, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3653, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6126, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0552, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6898, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4488, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8385, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8009, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5632, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9700, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4537, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0309, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2584, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7632, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6867, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5044, grad_fn=<NllLossBackward>)\n",
      "tensor(6.0558, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2321, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3013, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4664, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6386, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2295, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6590, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0540, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6817, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4324, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1662, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1556, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0557, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6965, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5825, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1032, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1365, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6975, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2010, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9253, grad_fn=<NllLossBackward>)\n",
      "tensor(5.8130, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7218, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5293, grad_fn=<NllLossBackward>)\n",
      "tensor(6.0491, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2067, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8219, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3447, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1433, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6532, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2071, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1135, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8945, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3729, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7615, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6089, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3096, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3833, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5091, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9582, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4101, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4634, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4465, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9679, grad_fn=<NllLossBackward>)\n",
      "tensor(6.3139, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2399, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2253, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9428, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5745, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0968, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0073, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7443, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0933, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4942, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6183, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0068, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9285, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1145, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2773, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2255, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7677, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1451, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7103, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0738, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7777, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5177, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5698, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0315, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5010, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7543, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0824, grad_fn=<NllLossBackward>)\n",
      "tensor(5.9885, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7024, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2947, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9561, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7439, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6031, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6471, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5514, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9584, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0428, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8966, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6312, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9565, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7241, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8876, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0563, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3700, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7991, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0850, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5977, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6789, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1858, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4155, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3978, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1828, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0073, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8770, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6110, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6294, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8589, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0526, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8371, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2557, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4713, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2764, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0888, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6388, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4318, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4309, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5582, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5463, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6955, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1622, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8668, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2468, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0757, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4478, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0993, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6488, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6799, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0300, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0708, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2323, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8315, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9720, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0591, grad_fn=<NllLossBackward>)\n",
      "tensor(5.8757, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1285, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9603, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5142, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7883, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3650, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8054, grad_fn=<NllLossBackward>)\n",
      "tensor(5.9184, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8825, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1847, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3638, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7057, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6740, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5576, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0504, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0322, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4464, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7664, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7623, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2736, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7026, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7686, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9443, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7722, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7809, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7956, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1394, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8223, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5148, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7818, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0224, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8403, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1442, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3018, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9360, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0078, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5030, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3685, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4213, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9369, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6623, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6997, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7694, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5138, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6723, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6740, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6865, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3099, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7314, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4727, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8022, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8863, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9115, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6321, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6743, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1022, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2103, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7965, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1119, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1755, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8211, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0021, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1951, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6662, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2548, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3183, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0344, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7713, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3122, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4058, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6863, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5012, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3443, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7126, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4090, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4898, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5702, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3715, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6169, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9671, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3543, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8791, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7532, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3587, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2088, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0606, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0587, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4925, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1446, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5286, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0303, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9990, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7596, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5819, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0705, grad_fn=<NllLossBackward>)\n",
      "tensor(5.9584, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3946, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8612, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6921, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4461, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1959, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4281, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7864, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9794, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2694, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5358, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9283, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8386, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8140, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9773, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2442, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5372, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3147, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9480, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4856, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5316, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6601, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3714, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5124, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2566, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0061, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0119, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3359, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3786, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9893, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9920, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9167, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3858, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3370, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8150, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1474, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4032, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3828, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4342, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2242, grad_fn=<NllLossBackward>)\n",
      "tensor(5.8435, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3317, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8533, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7114, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6357, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1721, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3932, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0662, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1666, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0439, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0509, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1018, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4257, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9564, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1946, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0470, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3054, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5812, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3904, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4122, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2600, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5123, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2075, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3700, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3397, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5012, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5244, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3061, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1176, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5337, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4779, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0450, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4578, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6135, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4400, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7830, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9100, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6900, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9191, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2099, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0417, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9838, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3575, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4274, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2512, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2856, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8237, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5611, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4644, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2857, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5607, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4504, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2833, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5966, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4568, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4014, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5267, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0502, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7393, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0947, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5004, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4724, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5030, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6331, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2565, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0298, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3728, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9292, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8488, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9449, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4395, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9855, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2054, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6039, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8336, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0131, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6954, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1754, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5307, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8155, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9293, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2942, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0350, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7694, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1225, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0216, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5255, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2625, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6335, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1866, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1510, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8572, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2123, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6530, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8796, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1682, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9392, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3816, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3064, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7331, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2746, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8251, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0454, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4433, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3749, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4528, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5500, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0501, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5491, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9311, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2169, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8757, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8923, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2068, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4038, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3675, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6490, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0980, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5196, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7201, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2827, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0363, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0124, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1955, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9080, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0593, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3128, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7618, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2739, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9137, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9380, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3850, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5578, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2997, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3338, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9564, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1721, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2606, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4381, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8828, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7358, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3946, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7669, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4812, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3978, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1992, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2117, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7574, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4294, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2347, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0916, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6466, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5250, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7709, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0620, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9327, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6274, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9344, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7731, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5016, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7183, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8291, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9203, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3145, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6015, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3922, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0423, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4704, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5515, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5176, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3509, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1576, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0386, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9045, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0841, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3825, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2540, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8633, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3177, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2778, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7323, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2718, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6334, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7709, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3717, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0817, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4692, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4820, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7797, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2255, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7714, grad_fn=<NllLossBackward>)\n",
      "tensor(6.0868, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9340, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5295, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9802, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5719, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2664, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0723, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8460, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2871, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7893, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6801, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3241, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4805, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3178, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7746, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4733, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1584, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9529, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6398, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0580, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0825, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7761, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6087, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9017, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9090, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8485, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4313, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5299, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3486, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4789, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1255, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7918, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8477, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3903, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3364, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5428, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0982, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8993, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3038, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7376, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5846, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9554, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8993, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0753, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5881, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5140, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6404, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4876, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3706, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0473, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1113, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1972, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7238, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6434, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4037, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9588, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1964, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9875, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4472, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9518, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9288, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2491, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6777, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5667, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3250, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2825, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9311, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2315, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8419, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3744, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8068, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7971, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6306, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7115, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4062, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7701, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7492, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3980, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5140, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0869, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1071, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9073, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9137, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5888, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2566, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0617, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2049, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8749, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4103, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8410, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7136, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2753, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3116, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0654, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3131, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8588, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3462, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9631, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9910, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4668, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3276, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5687, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7307, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4551, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4925, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4560, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1940, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4403, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0721, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3825, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7894, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5346, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1158, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6812, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6772, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2710, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5199, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0854, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6481, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7988, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4102, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9868, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4167, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4734, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7461, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3097, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4779, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4108, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7147, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5814, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4537, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6047, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4383, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9635, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7981, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-bec76b165f02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-65e46c66e77f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, loader)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder(20, 49, 1, dataset.topk + 1)\n",
    "train(encoder, decoder, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.1206,  0.0362, -0.2240,  0.2395, -0.0155, -0.0090,  0.0532,\n",
      "           0.1678,  0.1748,  0.0265]]], grad_fn=<ViewBackward>), tensor([[[-0.4015,  0.0666, -0.2668,  0.4081, -0.0563, -0.0465,  0.1942,\n",
      "           0.5321,  1.1125,  0.0434]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1229, -0.2185, -0.4701,  0.4513, -0.1097, -0.0544,  0.0641,\n",
      "           0.0982,  0.2417, -0.0161]]], grad_fn=<ViewBackward>), tensor([[[-0.3627, -0.7442, -0.5807,  0.9857, -0.2122, -0.3104,  0.3643,\n",
      "           0.7520,  0.9708, -0.0301]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1978, -0.4159, -0.3750,  0.3909, -0.0475, -0.1323,  0.1368,\n",
      "           0.1838,  0.1406,  0.0098]]], grad_fn=<ViewBackward>), tensor([[[-0.4194, -1.0675, -0.6118,  1.0504, -0.1487, -0.6193,  0.3856,\n",
      "           0.7525,  0.7458,  0.0186]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1297, -0.4165, -0.4629,  0.4339, -0.0909, -0.0684,  0.1993,\n",
      "           0.1639,  0.0761, -0.0210]]], grad_fn=<ViewBackward>), tensor([[[-0.3372, -1.0228, -0.7809,  0.8578, -0.2588, -0.4401,  0.6148,\n",
      "           0.7217,  0.2732, -0.0438]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.2057, -0.2723, -0.6325,  0.2759, -0.1528, -0.0230,  0.2012,\n",
      "           0.2058,  0.1101, -0.0336]]], grad_fn=<ViewBackward>), tensor([[[-0.4886, -1.1653, -0.9595,  0.8851, -0.3825, -0.1090,  0.6695,\n",
      "           0.6666,  0.3274, -0.0479]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1616, -0.3718, -0.6399,  0.3816, -0.0998, -0.0266,  0.2639,\n",
      "           0.1106,  0.0792, -0.0279]]], grad_fn=<ViewBackward>), tensor([[[-0.6110, -1.1068, -0.9943,  1.0083, -0.3380, -0.1011,  0.8243,\n",
      "           0.6769,  0.3943, -0.0436]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.2306, -0.2530, -0.4459,  0.3665, -0.1622, -0.0505,  0.2276,\n",
      "           0.2180,  0.0750,  0.0037]]], grad_fn=<ViewBackward>), tensor([[[-0.5373, -1.1525, -0.6722,  1.3338, -0.3317, -0.1731,  0.7921,\n",
      "           0.8031,  0.4711,  0.0061]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1683, -0.2653, -0.4759,  0.4070, -0.2023, -0.0212,  0.1875,\n",
      "           0.1545,  0.1794, -0.0740]]], grad_fn=<ViewBackward>), tensor([[[-0.4550, -1.2952, -0.6545,  1.3953, -0.5322, -0.0716,  0.6913,\n",
      "           0.6491,  0.4697, -0.1262]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.3001, -0.2225, -0.5200,  0.4889, -0.2593, -0.0178,  0.1262,\n",
      "           0.1460,  0.1495, -0.1430]]], grad_fn=<ViewBackward>), tensor([[[-0.6232, -0.8006, -0.7220,  1.2767, -0.4731, -0.0646,  0.7088,\n",
      "           0.7051,  0.6424, -0.2054]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.2582, -0.2524, -0.4896,  0.2897, -0.3073, -0.0135,  0.2436,\n",
      "           0.1528,  0.1264, -0.1601]]], grad_fn=<ViewBackward>), tensor([[[-0.4844, -1.1005, -0.6415,  1.1602, -0.5638, -0.0622,  0.7492,\n",
      "           0.7165,  0.4102, -0.2923]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 10\n",
    "embedding_dim = 49\n",
    "hidden_dim = 10\n",
    "num_layers = 1\n",
    "vocab_len = 29\n",
    "model = Decoder(hidden_dim, embedding_dim, num_layers, vocab_len)\n",
    "attention = Attention(embedding_dim, hidden_dim)\n",
    "encoder_output = torch.rand(batch_size, 512, 49)\n",
    "#inputs from embeddings [batch_size, seq_len, embedding_dim + hidden_dim]\n",
    "input_str = torch.rand(batch_size, seq_len, embedding_dim)\n",
    "hidden = torch.rand(2, num_layers, batch_size, hidden_dim)\n",
    "\n",
    "for i in range(input_str.shape[1]):\n",
    "    \n",
    "    attention_vector = attention(encoder_output, hidden)\n",
    "    lstm_input = torch.unsqueeze(torch.cat([input_str[:, i, :], attention_vector], dim=-1), 1)\n",
    "    output, hidden = model(lstm_input, hidden)\n",
    "    print(hidden)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 29])\n",
      "(tensor([[[ 0.1567, -0.3082,  0.2041, -0.2354,  0.0321, -0.0569,  0.0033,\n",
      "          -0.0245,  0.1133,  0.1696]]], grad_fn=<ViewBackward>), tensor([[[ 0.4709, -0.5611,  0.3368, -0.3590,  0.0493, -0.1002,  0.0086,\n",
      "          -0.0744,  0.4667,  0.2902]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
