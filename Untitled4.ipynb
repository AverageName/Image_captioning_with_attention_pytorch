{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import os\n",
    "import operator\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is nancy'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_caption(caption):\n",
    "    caption = ''.join([char for char in caption if not char in punctuation]).lower()\n",
    "    return caption\n",
    "prepare_caption('My name is Nancy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Flickr30k(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir, csv_file, transform=None, topk=5000):\n",
    "        self.df = pd.read_csv(os.path.join(root_dir, csv_file), delimiter='|')\n",
    "        self.df.iloc[19999][' comment_number'] = ' 4'\n",
    "        self.df.iloc[19999][' comment'] = ' A dog runs across the grass .'\n",
    "        self.captions = {}\n",
    "        self.vocab = Counter()\n",
    "        for idx, row in self.df.iterrows():\n",
    "            img_path = row['image_name']\n",
    "            caption = prepare_caption(row[' comment'])\n",
    "            if img_path not in self.captions:\n",
    "                self.captions.update({img_path: [caption]})\n",
    "            else:\n",
    "                self.captions[img_path].append(caption)\n",
    "                \n",
    "            for word in caption.split():\n",
    "                self.vocab[word] += 1\n",
    "                \n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.topk = topk\n",
    "        self.word2index = {word: index for index, (word, count) in enumerate(sorted(self.vocab.items(), key=operator.itemgetter(1), reverse=True)[:topk])}\n",
    "        self.index2word = {index: word for index, (word, count) in enumerate(sorted(self.vocab.items(), key=operator.itemgetter(1), reverse=True)[:topk])}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "    def __getitem__(self, x):\n",
    "        img_name = self.df.iloc[x * 5, 0]\n",
    "        img = Image.open(os.path.join(self.root_dir, 'flickr30k_images', img_name))\n",
    "        caption = sorted(self.captions[img_name], key=len)[-1]\n",
    "        caption_encoded = []\n",
    "        for word in caption.split():\n",
    "            if word not in self.word2index:\n",
    "                caption_encoded.append(self.topk)\n",
    "            else:\n",
    "                caption_encoded.append(self.word2index[word])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, caption, caption_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(tensors):\n",
    "    seq_len = max([tensor.shape[0] for tensor in tensors])\n",
    "    for i in range(len(tensors)):\n",
    "        if tensors[i].shape[0] < seq_len:\n",
    "            tensors[i] = torch.cat([tensors[i], torch.zeros(seq_len - tensors[i].shape[0])], dim=-1)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 2., 0.]), tensor([1., 3., 5.])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_seq([torch.Tensor([1, 2]), torch.Tensor([1, 3, 5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = [example[0] for example in batch]\n",
    "    captions = [example[1] for example in batch]\n",
    "    captions_encoded = [torch.Tensor(example[2]) for example in batch]\n",
    "    \n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    captions_encoded = pad_seq(captions_encoded)\n",
    "    captions_encoded = torch.stack(captions_encoded, dim=0)\n",
    "    \n",
    "    return imgs, captions, captions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Flickr30k('/mnt/c/Users/MAX/Downloads/flickr30k_images', 'results.csv', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=16, drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "    \n",
    "        self.model = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        \n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, features_dim, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(features_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, features, hidden):\n",
    "        features = self.linear1(features)\n",
    "        hidden = hidden[0]\n",
    "        \n",
    "        hidden = self.linear2(hidden)\n",
    "        #rint(features.shape)\n",
    "        score = torch.nn.functional.tanh(hidden.permute(1, 0, 2) + features)\n",
    "        \n",
    "        attention_weights = torch.nn.functional.softmax(self.linear3(score), dim=1)\n",
    "        \n",
    "        attention_vectors = attention_weights * score\n",
    "        attention_vectors = torch.mean(attention_vectors, dim=1)\n",
    "        \n",
    "        return attention_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, embedding_dim, num_layers, vocab_len):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_len, embedding_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim + embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_len)\n",
    "        self.attention = Attention(embedding_dim, hidden_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        if torch.cuda.is_available():\n",
    "            hidden = (torch.zeros((batch_size, self.num_layers, self.hidden_dim), device=torch.device('cuda')),\n",
    "                     torch.zeros((batch_size, self.num_layers, self.hidden_dim), device=torch.device('cuda'))\n",
    "                     )\n",
    "        else:\n",
    "            hidden = (torch.zeros((self.num_layers, batch_size, self.hidden_dim), device=torch.device('cpu')),\n",
    "                     torch.zeros((self.num_layers, batch_size, self.hidden_dim), device=torch.device('cpu'))\n",
    "                     )\n",
    "            \n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x, enc_embed, hidden):\n",
    "        embeds = self.embeddings(x)\n",
    "        embeds = torch.unsqueeze(embeds, 1)\n",
    "        #print(embeds.shape)\n",
    "        attention_vectors = self.attention(enc_embed, hidden)\n",
    "        #print(attention_vectors.shape)\n",
    "        attention_vectors = torch.unsqueeze(attention_vectors, 1)\n",
    "        x = torch.cat([embeds, attention_vectors], dim=-1)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.linear(x)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_with_mask(pred_vec, target_vec, loss_func):\n",
    "    indices = torch.nonzero(target_vec)\n",
    "    pred_vec = torch.squeeze(pred_vec[indices])\n",
    "    target_vec = torch.squeeze(target_vec[indices])\n",
    "    return loss_func(pred_vec, target_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, loader, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(decoder.parameters())\n",
    "    for epoch in range(num_epochs):\n",
    "        for idx, data in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            inputs = data[0].float()\n",
    "            batch_size = inputs.shape[0]\n",
    "        \n",
    "            enc_embeds = encoder(inputs)\n",
    "            enc_embeds = enc_embeds.view(batch_size, encoder.model.inplanes, -1)\n",
    "            labels = data[2].long()\n",
    "            hidden = decoder.init_hidden(batch_size)\n",
    "        \n",
    "            outputs = []\n",
    "            for i in range(labels.shape[1]):\n",
    "                output, hidden = decoder(labels[:, i], enc_embeds, hidden)\n",
    "                outputs.append(output)\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "            outputs = outputs.view(-1, dataset.topk + 1)\n",
    "            labels = labels.view(-1)\n",
    "            loss = loss_with_mask(outputs, labels, criterion)\n",
    "            if idx % 5 == 4:\n",
    "                print(\"Num steps: \", idx + 1, \" Loss: \", loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps:  5  Loss:  tensor(8.4885, grad_fn=<NllLossBackward>)\n",
      "Num steps:  10  Loss:  tensor(8.4329, grad_fn=<NllLossBackward>)\n",
      "Num steps:  15  Loss:  tensor(8.3629, grad_fn=<NllLossBackward>)\n",
      "Num steps:  20  Loss:  tensor(8.2771, grad_fn=<NllLossBackward>)\n",
      "Num steps:  25  Loss:  tensor(8.1355, grad_fn=<NllLossBackward>)\n",
      "Num steps:  30  Loss:  tensor(8.0506, grad_fn=<NllLossBackward>)\n",
      "Num steps:  35  Loss:  tensor(7.7341, grad_fn=<NllLossBackward>)\n",
      "Num steps:  40  Loss:  tensor(7.4051, grad_fn=<NllLossBackward>)\n",
      "Num steps:  45  Loss:  tensor(7.0200, grad_fn=<NllLossBackward>)\n",
      "Num steps:  50  Loss:  tensor(6.6714, grad_fn=<NllLossBackward>)\n",
      "Num steps:  55  Loss:  tensor(6.4411, grad_fn=<NllLossBackward>)\n",
      "Num steps:  60  Loss:  tensor(6.3063, grad_fn=<NllLossBackward>)\n",
      "Num steps:  65  Loss:  tensor(6.0539, grad_fn=<NllLossBackward>)\n",
      "Num steps:  70  Loss:  tensor(6.3910, grad_fn=<NllLossBackward>)\n",
      "Num steps:  75  Loss:  tensor(6.2321, grad_fn=<NllLossBackward>)\n",
      "Num steps:  80  Loss:  tensor(6.2283, grad_fn=<NllLossBackward>)\n",
      "Num steps:  85  Loss:  tensor(5.9619, grad_fn=<NllLossBackward>)\n",
      "Num steps:  90  Loss:  tensor(6.1139, grad_fn=<NllLossBackward>)\n",
      "Num steps:  95  Loss:  tensor(5.7972, grad_fn=<NllLossBackward>)\n",
      "Num steps:  100  Loss:  tensor(5.5424, grad_fn=<NllLossBackward>)\n",
      "Num steps:  105  Loss:  tensor(6.0844, grad_fn=<NllLossBackward>)\n",
      "Num steps:  110  Loss:  tensor(5.5875, grad_fn=<NllLossBackward>)\n",
      "Num steps:  115  Loss:  tensor(5.3346, grad_fn=<NllLossBackward>)\n",
      "Num steps:  120  Loss:  tensor(5.5448, grad_fn=<NllLossBackward>)\n",
      "Num steps:  125  Loss:  tensor(5.7243, grad_fn=<NllLossBackward>)\n",
      "Num steps:  130  Loss:  tensor(5.8259, grad_fn=<NllLossBackward>)\n",
      "Num steps:  135  Loss:  tensor(5.5022, grad_fn=<NllLossBackward>)\n",
      "Num steps:  140  Loss:  tensor(5.0248, grad_fn=<NllLossBackward>)\n",
      "Num steps:  145  Loss:  tensor(5.3734, grad_fn=<NllLossBackward>)\n",
      "Num steps:  150  Loss:  tensor(4.9910, grad_fn=<NllLossBackward>)\n",
      "Num steps:  155  Loss:  tensor(4.9636, grad_fn=<NllLossBackward>)\n",
      "Num steps:  160  Loss:  tensor(5.1599, grad_fn=<NllLossBackward>)\n",
      "Num steps:  165  Loss:  tensor(5.1104, grad_fn=<NllLossBackward>)\n",
      "Num steps:  170  Loss:  tensor(5.1485, grad_fn=<NllLossBackward>)\n",
      "Num steps:  175  Loss:  tensor(4.6937, grad_fn=<NllLossBackward>)\n",
      "Num steps:  180  Loss:  tensor(5.4271, grad_fn=<NllLossBackward>)\n",
      "Num steps:  185  Loss:  tensor(5.0377, grad_fn=<NllLossBackward>)\n",
      "Num steps:  190  Loss:  tensor(4.9913, grad_fn=<NllLossBackward>)\n",
      "Num steps:  195  Loss:  tensor(5.0144, grad_fn=<NllLossBackward>)\n",
      "Num steps:  200  Loss:  tensor(4.8123, grad_fn=<NllLossBackward>)\n",
      "Num steps:  205  Loss:  tensor(4.3728, grad_fn=<NllLossBackward>)\n",
      "Num steps:  210  Loss:  tensor(5.0546, grad_fn=<NllLossBackward>)\n",
      "Num steps:  215  Loss:  tensor(4.5043, grad_fn=<NllLossBackward>)\n",
      "Num steps:  220  Loss:  tensor(4.4727, grad_fn=<NllLossBackward>)\n",
      "Num steps:  225  Loss:  tensor(4.4801, grad_fn=<NllLossBackward>)\n",
      "Num steps:  230  Loss:  tensor(4.2089, grad_fn=<NllLossBackward>)\n",
      "Num steps:  235  Loss:  tensor(4.5032, grad_fn=<NllLossBackward>)\n",
      "Num steps:  240  Loss:  tensor(4.5913, grad_fn=<NllLossBackward>)\n",
      "Num steps:  245  Loss:  tensor(4.5129, grad_fn=<NllLossBackward>)\n",
      "Num steps:  250  Loss:  tensor(4.0209, grad_fn=<NllLossBackward>)\n",
      "Num steps:  255  Loss:  tensor(4.3196, grad_fn=<NllLossBackward>)\n",
      "Num steps:  260  Loss:  tensor(4.0531, grad_fn=<NllLossBackward>)\n",
      "Num steps:  265  Loss:  tensor(4.3347, grad_fn=<NllLossBackward>)\n",
      "Num steps:  270  Loss:  tensor(4.0909, grad_fn=<NllLossBackward>)\n",
      "Num steps:  275  Loss:  tensor(3.9772, grad_fn=<NllLossBackward>)\n",
      "Num steps:  280  Loss:  tensor(4.0700, grad_fn=<NllLossBackward>)\n",
      "Num steps:  285  Loss:  tensor(4.2774, grad_fn=<NllLossBackward>)\n",
      "Num steps:  290  Loss:  tensor(4.2592, grad_fn=<NllLossBackward>)\n",
      "Num steps:  295  Loss:  tensor(4.2202, grad_fn=<NllLossBackward>)\n",
      "Num steps:  300  Loss:  tensor(3.9218, grad_fn=<NllLossBackward>)\n",
      "Num steps:  305  Loss:  tensor(3.9460, grad_fn=<NllLossBackward>)\n",
      "Num steps:  310  Loss:  tensor(4.1120, grad_fn=<NllLossBackward>)\n",
      "Num steps:  315  Loss:  tensor(3.8377, grad_fn=<NllLossBackward>)\n",
      "Num steps:  320  Loss:  tensor(4.4532, grad_fn=<NllLossBackward>)\n",
      "Num steps:  325  Loss:  tensor(3.4797, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-18a0e682a7c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-d60dd38d8ac6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, loader, num_epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Num steps: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder(50, 49, 1, dataset.topk + 1)\n",
    "train(encoder, decoder, data_loader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inputs, labels):\n",
    "    batch_size = inputs.shape[0]\n",
    "    enc_embeds = encoder(inputs)\n",
    "    enc_embeds = enc_embeds.view(batch_size, encoder.model.inplanes, -1)\n",
    "    hidden = decoder.init_hidden(batch_size)\n",
    "    \n",
    "    #print(hidden[0].shape)\n",
    "    outputs = []\n",
    "    for i in range(labels.shape[1]):\n",
    "        output, hidden = decoder(labels[:, i], enc_embeds, hidden)\n",
    "        outputs.append(output)\n",
    "    outputs = torch.cat(outputs, dim=1)\n",
    "    outputs = outputs.view(-1, dataset.topk + 1)\n",
    "    outputs = nn.functional.softmax(outputs, dim=-1)\n",
    "    words_idx = torch.argmax(outputs, dim=-1).numpy()\n",
    "    print(words_idx)\n",
    "    words = ' '.join([dataset.index2word[word_idx] if word_idx < 5000 else \"Unknown\" for word_idx in words_idx])\n",
    "    return outputs, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_loader = DataLoader(dataset, batch_size=1, drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "torch.Size([1, 1, 50])\n",
      "Attn:  torch.Size([1, 1, 50])\n",
      "Embeds:  torch.Size([1, 1, 49])\n",
      "[2923 1042 1707  677 2274  479   80 1753 1382 1422 1765 1765 1286 3847\n",
      "  594 4292]\n",
      "mailbox sport trunk christmas meadow cleaning stands kisses displaying microphones skiers skiers crosses segway cliff yankees\n",
      "tensor([[  10.,   17.,  320.,    8., 2103.,  106.,  182.,   14.,   59.,  154.,\n",
      "           21.,  319.,   69.,    1.,    2.,  485.]])\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(val_data_loader)\n",
    "inputs, _, labels = iterator.next()\n",
    "#print(inputs.shape[0])\n",
    "print(evaluate(inputs, labels.long())[1])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
