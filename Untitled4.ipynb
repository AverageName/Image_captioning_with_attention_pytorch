{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "df = pd.read_csv('/mnt/c/Users/MAX/Downloads/flickr30k_images/results.csv', delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>2199200615.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>A dog running on green grass with its mouth o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>2199200615.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>a white dog is running with its mouth open ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>2199200615.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>A white , black , and brown dog runs in a fie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>2199200615.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A dog runs across the grassy field .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>2199200615.jpg</td>\n",
       "      <td>4   A dog runs across the grass .</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_name                      comment_number  \\\n",
       "19995  2199200615.jpg                                   0   \n",
       "19996  2199200615.jpg                                   1   \n",
       "19997  2199200615.jpg                                   2   \n",
       "19998  2199200615.jpg                                   3   \n",
       "19999  2199200615.jpg   4   A dog runs across the grass .   \n",
       "\n",
       "                                                 comment  \n",
       "19995   A dog running on green grass with its mouth o...  \n",
       "19996   a white dog is running with its mouth open ac...  \n",
       "19997   A white , black , and brown dog runs in a fie...  \n",
       "19998               A dog runs across the grassy field .  \n",
       "19999                                                NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(df['image_name'] == '2199200615.jpg') & (len(df[' comment_number'].tolist()) > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[19999]['comment'] = ' A dog runcs across the grass .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({' 0': 31783, ' 1': 31783, ' 2': 31783, ' 3': 31783, ' 4': 31783})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(df[' comment_number'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l': 2, 'k': 3}\n",
      "{'l': 2, 'k': 3}\n"
     ]
    }
   ],
   "source": [
    "kek = {}\n",
    "kek.update({'l': 2, 'k':3})\n",
    "print(kek)\n",
    "kek.update({'l': 2, 'k':3})\n",
    "print(kek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_caption(caption):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Flickr30k(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir, csv_file, transform=None):\n",
    "        self.df = pd.read_csv(os.path.join(root_dir, csv_file), delimiter='|')\n",
    "        self.df.iloc[19999][' comment_number'] = ' 4'\n",
    "        self.df.iloc[19999]['comment'] = ' A dog runs across the grass .'\n",
    "        self.captions = {}\n",
    "        self.vocab = {'<sos>': 0, '<eos>' : 1}\n",
    "        for idx, row in self.df.iterrows():\n",
    "            \n",
    "            if row['image_name'] not in self.captions:\n",
    "                self.captions.update({row['image_name']: [row[' comment']]})\n",
    "            else:\n",
    "                self.captions[row['image_name']].append(row[' comment'])\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "    def __getitem__(self, x):\n",
    "        img_name = self.df.iloc[x * 5, 0]\n",
    "        print(img_name)\n",
    "        img = Image.open(os.path.join(self.root_dir, 'flickr30k_images', img_name))\n",
    "        caption = sorted(self.captions[img_name], key=len)[-1]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Flickr30k('/mnt/c/Users/MAX/Downloads/flickr30k_images', 'results.csv', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000092795.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.1179, -2.0837, -2.0665,  ...,  1.6324,  1.6838,  1.6667],\n",
       "          [-2.0494, -2.0665, -2.0323,  ...,  1.7865,  1.5125,  1.6153],\n",
       "          [-2.0665, -2.0494, -2.0323,  ...,  2.0263,  1.8550,  1.5468],\n",
       "          ...,\n",
       "          [ 0.7248,  0.9646,  0.1083,  ..., -0.3027,  0.1768, -0.2856],\n",
       "          [ 1.3413,  0.3481,  0.1597,  ...,  0.5707,  0.5536,  0.5707],\n",
       "          [ 0.3481,  1.0502,  1.0331,  ...,  0.5536,  1.1529, -0.3027]],\n",
       " \n",
       "         [[-1.9307, -1.9307, -1.9482,  ...,  2.2360,  2.3936,  2.3235],\n",
       "          [-1.8957, -1.9482, -1.9132,  ...,  2.3936,  2.2535,  2.3936],\n",
       "          [-1.9832, -1.9657, -1.9482,  ...,  2.4286,  2.4286,  2.2535],\n",
       "          ...,\n",
       "          [ 1.0630,  1.2556,  0.6779,  ...,  0.7304,  0.9755,  0.8179],\n",
       "          [ 1.7983,  1.0280,  0.7654,  ...,  1.2556,  1.2206,  1.3957],\n",
       "          [ 0.7129,  1.6583,  1.7108,  ...,  1.2206,  1.4657,  0.1702]],\n",
       " \n",
       "         [[-1.7347, -1.7173, -1.7347,  ...,  2.5354,  2.6400,  2.5529],\n",
       "          [-1.6824, -1.7347, -1.6999,  ...,  2.5529,  2.6226,  2.6400],\n",
       "          [-1.7522, -1.7347, -1.7173,  ...,  2.6400,  2.5703,  2.5529],\n",
       "          ...,\n",
       "          [ 0.7576,  0.4788, -0.0615,  ..., -0.3055,  0.1651, -0.5495],\n",
       "          [ 0.8274,  0.1999,  0.2348,  ...,  0.4439,  0.3568,  0.2348],\n",
       "          [-0.2881,  1.1934,  0.8448,  ...,  0.3219,  0.8622, -0.6715]]]),\n",
       " ' Two young guys with shaggy hair look at their hands while hanging out in the yard .')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "    \n",
    "        self.model = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        \n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 49])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((1, 3, 224, 224))\n",
    "model = Encoder()\n",
    "model.eval()\n",
    "output = model(x)\n",
    "output.view(-1, 512, output.shape[-1] * output.shape[-2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, features_dim, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(features_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, features, hidden):\n",
    "        features = self.linear1(features)\n",
    "        hidden = hidden[0]\n",
    "        \n",
    "        hidden = self.linear2(hidden)\n",
    "        \n",
    "        score = torch.nn.functional.tanh(hidden + features)\n",
    "        \n",
    "        attention_weights = torch.nn.functional.softmax(self.linear3(score), dim=1)\n",
    "        \n",
    "        attention_vectors = attention_weights * score\n",
    "        attention_vectors = torch.mean(attention_vectors, dim=1)\n",
    "        \n",
    "        return attention_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, embedding_dim, num_layers, vocab_len):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_dim + embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_len)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.linear(x)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.1206,  0.0362, -0.2240,  0.2395, -0.0155, -0.0090,  0.0532,\n",
      "           0.1678,  0.1748,  0.0265]]], grad_fn=<ViewBackward>), tensor([[[-0.4015,  0.0666, -0.2668,  0.4081, -0.0563, -0.0465,  0.1942,\n",
      "           0.5321,  1.1125,  0.0434]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1229, -0.2185, -0.4701,  0.4513, -0.1097, -0.0544,  0.0641,\n",
      "           0.0982,  0.2417, -0.0161]]], grad_fn=<ViewBackward>), tensor([[[-0.3627, -0.7442, -0.5807,  0.9857, -0.2122, -0.3104,  0.3643,\n",
      "           0.7520,  0.9708, -0.0301]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1978, -0.4159, -0.3750,  0.3909, -0.0475, -0.1323,  0.1368,\n",
      "           0.1838,  0.1406,  0.0098]]], grad_fn=<ViewBackward>), tensor([[[-0.4194, -1.0675, -0.6118,  1.0504, -0.1487, -0.6193,  0.3856,\n",
      "           0.7525,  0.7458,  0.0186]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1297, -0.4165, -0.4629,  0.4339, -0.0909, -0.0684,  0.1993,\n",
      "           0.1639,  0.0761, -0.0210]]], grad_fn=<ViewBackward>), tensor([[[-0.3372, -1.0228, -0.7809,  0.8578, -0.2588, -0.4401,  0.6148,\n",
      "           0.7217,  0.2732, -0.0438]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.2057, -0.2723, -0.6325,  0.2759, -0.1528, -0.0230,  0.2012,\n",
      "           0.2058,  0.1101, -0.0336]]], grad_fn=<ViewBackward>), tensor([[[-0.4886, -1.1653, -0.9595,  0.8851, -0.3825, -0.1090,  0.6695,\n",
      "           0.6666,  0.3274, -0.0479]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1616, -0.3718, -0.6399,  0.3816, -0.0998, -0.0266,  0.2639,\n",
      "           0.1106,  0.0792, -0.0279]]], grad_fn=<ViewBackward>), tensor([[[-0.6110, -1.1068, -0.9943,  1.0083, -0.3380, -0.1011,  0.8243,\n",
      "           0.6769,  0.3943, -0.0436]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.2306, -0.2530, -0.4459,  0.3665, -0.1622, -0.0505,  0.2276,\n",
      "           0.2180,  0.0750,  0.0037]]], grad_fn=<ViewBackward>), tensor([[[-0.5373, -1.1525, -0.6722,  1.3338, -0.3317, -0.1731,  0.7921,\n",
      "           0.8031,  0.4711,  0.0061]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1683, -0.2653, -0.4759,  0.4070, -0.2023, -0.0212,  0.1875,\n",
      "           0.1545,  0.1794, -0.0740]]], grad_fn=<ViewBackward>), tensor([[[-0.4550, -1.2952, -0.6545,  1.3953, -0.5322, -0.0716,  0.6913,\n",
      "           0.6491,  0.4697, -0.1262]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.3001, -0.2225, -0.5200,  0.4889, -0.2593, -0.0178,  0.1262,\n",
      "           0.1460,  0.1495, -0.1430]]], grad_fn=<ViewBackward>), tensor([[[-0.6232, -0.8006, -0.7220,  1.2767, -0.4731, -0.0646,  0.7088,\n",
      "           0.7051,  0.6424, -0.2054]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.2582, -0.2524, -0.4896,  0.2897, -0.3073, -0.0135,  0.2436,\n",
      "           0.1528,  0.1264, -0.1601]]], grad_fn=<ViewBackward>), tensor([[[-0.4844, -1.1005, -0.6415,  1.1602, -0.5638, -0.0622,  0.7492,\n",
      "           0.7165,  0.4102, -0.2923]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 10\n",
    "embedding_dim = 49\n",
    "hidden_dim = 10\n",
    "num_layers = 1\n",
    "vocab_len = 29\n",
    "model = Decoder(hidden_dim, embedding_dim, num_layers, vocab_len)\n",
    "attention = Attention(embedding_dim, hidden_dim)\n",
    "encoder_output = torch.rand(batch_size, 512, 49)\n",
    "#inputs from embeddings [batch_size, seq_len, embedding_dim + hidden_dim]\n",
    "input_str = torch.rand(batch_size, seq_len, embedding_dim)\n",
    "hidden = torch.rand(2, num_layers, batch_size, hidden_dim)\n",
    "\n",
    "for i in range(input_str.shape[1]):\n",
    "    \n",
    "    attention_vector = attention(encoder_output, hidden)\n",
    "    lstm_input = torch.unsqueeze(torch.cat([input_str[:, i, :], attention_vector], dim=-1), 1)\n",
    "    output, hidden = model(lstm_input, hidden)\n",
    "    print(hidden)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 29])\n",
      "(tensor([[[ 0.1567, -0.3082,  0.2041, -0.2354,  0.0321, -0.0569,  0.0033,\n",
      "          -0.0245,  0.1133,  0.1696]]], grad_fn=<ViewBackward>), tensor([[[ 0.4709, -0.5611,  0.3368, -0.3590,  0.0493, -0.1002,  0.0086,\n",
      "          -0.0744,  0.4667,  0.2902]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
