{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import os\n",
    "import operator\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is nancy'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_caption(caption):\n",
    "    caption = ''.join([char for char in caption if not char in punctuation]).lower()\n",
    "    return caption\n",
    "prepare_caption('My name is Nancy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Flickr30k(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir, csv_file, transform=None, topk=5000):\n",
    "        self.df = pd.read_csv(os.path.join(root_dir, csv_file), delimiter='|')\n",
    "        self.df.iloc[19999][' comment_number'] = ' 4'\n",
    "        self.df.iloc[19999][' comment'] = ' A dog runs across the grass .'\n",
    "        self.captions = {}\n",
    "        self.vocab = Counter()\n",
    "        for idx, row in self.df.iterrows():\n",
    "            img_path = row['image_name']\n",
    "            caption = prepare_caption(row[' comment'])\n",
    "            if img_path not in self.captions:\n",
    "                self.captions.update({img_path: [caption]})\n",
    "            else:\n",
    "                self.captions[img_path].append(caption)\n",
    "                \n",
    "            for word in caption.split():\n",
    "                self.vocab[word] += 1\n",
    "                \n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.topk = topk\n",
    "        self.word2index = {word: index for index, (word, count) in enumerate(sorted(self.vocab.items(), key=operator.itemgetter(1), reverse=True)[:topk])}\n",
    "        self.index2word = {index: word for index, (word, count) in enumerate(sorted(self.vocab.items(), key=operator.itemgetter(1), reverse=True)[:topk])}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "    def __getitem__(self, x):\n",
    "        img_name = self.df.iloc[x * 5, 0]\n",
    "        img = Image.open(os.path.join(self.root_dir, 'flickr30k_images', img_name))\n",
    "        caption = sorted(self.captions[img_name], key=len)[-1]\n",
    "        caption_encoded = []\n",
    "        for word in caption.split():\n",
    "            if word not in self.word2index:\n",
    "                caption_encoded.append(self.topk)\n",
    "            else:\n",
    "                caption_encoded.append(self.word2index[word])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, caption, caption_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(tensors):\n",
    "    seq_len = max([tensor.shape[0] for tensor in tensors])\n",
    "    for i in range(len(tensors)):\n",
    "        if tensors[i].shape[0] < seq_len:\n",
    "            tensors[i] = torch.cat([tensors[i], torch.zeros(seq_len - tensors[i].shape[0])], dim=-1)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = [example[0] for example in batch]\n",
    "    captions = [example[1] for example in batch]\n",
    "    captions_encoded = [torch.Tensor(example[2]) for example in batch]\n",
    "    \n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    captions_encoded = pad_seq(captions_encoded)\n",
    "    captions_encoded = torch.stack(captions_encoded, dim=0)\n",
    "    \n",
    "    return imgs, captions, captions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Flickr30k('/mnt/c/Users/MAX/Downloads/flickr30k_images', 'results.csv', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=2, drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000092795.jpg\n",
      " two young guys with shaggy hair look at their hands while hanging out in the yard  [10, 17, 320, 8, 2103, 106, 182, 14, 59, 154, 21, 319, 69, 1, 2, 485]\n",
      "10002456.jpg\n",
      " several men in hard hats are operating a giant pulley system  [115, 27, 1, 326, 270, 11, 1306, 0, 806, 3842, 2628]\n",
      "0 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "1000268201.jpg\n",
      " a child in a pink dress is climbing up a set of stairs in an entry way  [0, 46, 1, 0, 83, 112, 6, 240, 45, 0, 355, 7, 400, 1, 15, 5000, 649]\n",
      "1000344755.jpg\n",
      " someone in a blue shirt and hat is standing on stair and leaning against a window  [273, 1, 0, 23, 19, 4, 60, 6, 29, 3, 2831, 4, 360, 221, 0, 228]\n",
      "1 torch.Size([2, 3, 224, 224]) torch.Size([2, 17])\n",
      "1000366164.jpg\n",
      " two men  one in a gray shirt  one in a black shirt  standing near a stove  [10, 27, 39, 1, 0, 116, 19, 39, 1, 0, 20, 19, 29, 77, 0, 1392]\n",
      "1000523639.jpg\n",
      " two people in the photo are playing the guitar and the other is poking at him  [10, 13, 1, 2, 356, 11, 31, 2, 122, 4, 2, 61, 6, 3603, 14, 121]\n",
      "2 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "1000919630.jpg\n",
      " a man sits in a chair while holding a large stuffed animal of a lion  [0, 5, 89, 1, 0, 231, 21, 38, 0, 51, 875, 629, 7, 0, 2407]\n",
      "10010052.jpg\n",
      " a girl is on rollerskates talking on her cellphone standing in a parking lot  [0, 25, 6, 3, 1928, 117, 3, 36, 283, 29, 1, 0, 572, 369]\n",
      "3 torch.Size([2, 3, 224, 224]) torch.Size([2, 15])\n",
      "1001465944.jpg\n",
      " an asian man wearing a black suit stands near a darkhaired woman and a brownhaired woman  [15, 98, 5, 16, 0, 20, 185, 80, 77, 0, 623, 9, 4, 0, 1116, 9]\n",
      "1001545525.jpg\n",
      " two men in germany jumping over a rail at the same time without shirts  [10, 27, 1, 5000, 86, 66, 0, 646, 14, 2, 615, 591, 1057, 251]\n",
      "4 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "1001573224.jpg\n",
      " five ballet dancers caught mid jump in a dancing studio with sunlight coming through a window  [262, 1393, 788, 1165, 1820, 288, 1, 0, 234, 1406, 8, 1862, 687, 57, 0, 228]\n",
      "1001633352.jpg\n",
      " three young men and a young woman wearing sneakers are leaping in midair at the top of a flight of concrete stairs  [42, 17, 27, 4, 0, 17, 9, 16, 1265, 11, 744, 1, 415, 14, 2, 95, 7, 0, 1206, 7, 410, 400]\n",
      "5 torch.Size([2, 3, 224, 224]) torch.Size([2, 22])\n",
      "1001773457.jpg\n",
      " a black dog and a white dog with brown spots are staring at each other in the street  [0, 20, 30, 4, 0, 18, 30, 8, 56, 2569, 11, 582, 14, 163, 61, 1, 2, 32]\n",
      "1001896054.jpg\n",
      " a man with reflective safety clothes and ear protection drives a john deere tractor on a road  [0, 5, 8, 1295, 490, 297, 4, 1079, 2570, 998, 0, 2935, 4087, 1052, 3, 0, 137]\n",
      "6 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "100197432.jpg\n",
      " some women are standing in front of a bus with buildings behind it  [67, 43, 11, 29, 1, 35, 7, 0, 302, 8, 452, 84, 114]\n",
      "100207720.jpg\n",
      " a young woman with dark hair and wearing glasses is putting white powder on a cake using a sifter  [0, 17, 9, 8, 173, 106, 4, 16, 147, 6, 426, 18, 3604, 3, 0, 679, 201, 0, 5000]\n",
      "7 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "1002674143.jpg\n",
      " a small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it  [0, 63, 25, 1, 2, 90, 130, 8, 5000, 1, 35, 7, 0, 18, 1898, 8, 0, 1394, 3, 114]\n",
      "1003163366.jpg\n",
      " a man sleeping on a bench outside with a white and black dog sitting next to him  [0, 5, 330, 3, 0, 140, 52, 8, 0, 18, 4, 20, 30, 26, 64, 12, 121]\n",
      "8 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "1003420127.jpg\n",
      " a group of adults  inside a home  sitting on chairs arranged in a circle  playing a type of musical instruments  [0, 33, 7, 331, 254, 0, 549, 26, 3, 388, 3315, 1, 0, 791, 31, 0, 777, 7, 636, 395]\n",
      "1003428081.jpg\n",
      " two women  both wearing glasses  are playing clarinets and an elderly woman is playing a stringed instrument  [10, 43, 349, 16, 147, 11, 31, 5000, 4, 15, 233, 9, 6, 31, 0, 2151, 471]\n",
      "9 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "100444898.jpg\n",
      " a person in gray stands alone on a structure outdoors in the dark  [0, 54, 1, 116, 80, 682, 3, 0, 505, 291, 1, 2, 173]\n",
      "1005216151.jpg\n",
      " a man in a white tshirt looks toward the camera surrounded by a crowd near a metro station  [0, 5, 1, 0, 18, 183, 94, 430, 2, 96, 280, 41, 0, 79, 77, 0, 2438, 478]\n",
      "10 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "100577935.jpg\n",
      " a man with a goatee in a black shirt and white latex gloves is using a tattoo gun to place a tattoo on someone s back  [0, 5, 8, 0, 2791, 1, 0, 20, 19, 4, 18, 4219, 500, 6, 201, 0, 794, 848, 12, 637, 0, 794, 3, 273, 103, 155]\n",
      "1006452823.jpg\n",
      " two children  a girl and a boy are practicing their writing  [10, 55, 0, 25, 4, 0, 28, 11, 544, 59, 628]\n",
      "11 torch.Size([2, 3, 224, 224]) torch.Size([2, 26])\n",
      "100652400.jpg\n",
      " a man in a blue hard hat and orange safety vest stands in an intersection while holding a flag  [0, 5, 1, 0, 23, 326, 60, 4, 76, 490, 298, 80, 1, 15, 1202, 21, 38, 0, 377]\n",
      "1007129816.jpg\n",
      " the man with pierced ears is wearing glasses and an orange hat  [2, 5, 8, 4550, 1114, 6, 16, 147, 4, 15, 76, 60]\n",
      "12 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "100716317.jpg\n",
      " a person with long gray hair has a beret with beige and white wearing a blue raincoat is painting a marketplace scenery surrounded by other artists and paintings  [0, 54, 8, 186, 116, 106, 108, 0, 2152, 8, 807, 4, 18, 16, 0, 23, 2439, 6, 313, 0, 1191, 1797, 280, 41, 61, 2027, 4, 2470]\n",
      "1007205537.jpg\n",
      " a man stands on one foot while holding on to a waste basket  [0, 5, 80, 3, 39, 730, 21, 38, 3, 12, 0, 5000, 597]\n",
      "13 torch.Size([2, 3, 224, 224]) torch.Size([2, 28])\n",
      "1007320043.jpg\n",
      " a small child grips onto the red ropes at the playground  [0, 63, 46, 5000, 335, 2, 24, 1476, 14, 2, 457]\n",
      "100759042.jpg\n",
      " young man in jacket holding a toothpick with something on the end of it [17, 5, 1, 73, 38, 0, 5000, 8, 113, 3, 2, 887, 7, 114]\n",
      "14 torch.Size([2, 3, 224, 224]) torch.Size([2, 14])\n",
      "10082347.jpg\n",
      " young blond man in a blue and yellow jacket smiling while standing in front of a net  [17, 150, 5, 1, 0, 23, 4, 53, 73, 127, 21, 29, 1, 35, 7, 0, 501]\n",
      "10082348.jpg\n",
      " a man with a baseball cap and black jacket stands in a bathroom while holding a coffee mug  [0, 5, 8, 0, 166, 229, 4, 20, 73, 80, 1, 0, 1455, 21, 38, 0, 583, 2327]\n",
      "15 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "100845130.jpg\n",
      " five people walking with a multicolored sky in the background  [262, 13, 37, 8, 0, 796, 437, 1, 2, 82]\n",
      "10090841.jpg\n",
      " a man with black hair sits in a restaurant with a glass of beer  [0, 5, 8, 20, 106, 89, 1, 0, 249, 8, 0, 398, 7, 493]\n",
      "16 torch.Size([2, 3, 224, 224]) torch.Size([2, 14])\n",
      "1009434119.jpg\n",
      " a black and white dog is running in a grassy garden surrounded by a white fence  [0, 20, 4, 18, 30, 6, 71, 1, 0, 284, 706, 280, 41, 0, 18, 259]\n",
      "1009692167.jpg\n",
      " an officer in a reflective vest stands at the front of his van with his dog  [15, 722, 1, 0, 1295, 298, 80, 14, 2, 35, 7, 22, 893, 8, 22, 30]\n",
      "17 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "101001624.jpg\n",
      " the  white out  conditions of snow on the ground seem to almost obliverate the details of a man dressed for the cold weather in a heavy jacket and red hat riding a bicycle in a suburban neighborhood  [2, 18, 69, 4220, 7, 91, 3, 2, 169, 1798, 12, 1929, 5000, 2, 5000, 7, 0, 5, 68, 49, 2, 936, 1373, 1, 0, 881, 73, 4, 24, 60, 74, 0, 153, 1, 0, 2540, 1366]\n",
      "1010031975.jpg\n",
      " small group of 5 white males in white suits hanging out by the back of a van in a parking lot talking  [63, 33, 7, 1108, 18, 704, 1, 18, 525, 319, 69, 41, 2, 155, 7, 0, 893, 1, 0, 572, 369, 117]\n",
      "18 torch.Size([2, 3, 224, 224]) torch.Size([2, 37])\n",
      "1010087179.jpg\n",
      " a tan man with a backwards hat looks at the camera while walking through a factory  [0, 260, 5, 8, 0, 1203, 60, 94, 14, 2, 96, 21, 37, 57, 0, 1562]\n",
      "1010087623.jpg\n",
      " a caucasian man wearing a shortsleeved black shirt and a darkskinned woman wearing a sleeveless dress are working at a conveyor  [0, 1209, 5, 16, 0, 2028, 20, 19, 4, 0, 1081, 9, 16, 0, 1367, 112, 11, 109, 14, 0, 3729]\n",
      "19 torch.Size([2, 3, 224, 224]) torch.Size([2, 21])\n",
      "10101477.jpg\n",
      " man wearing a blue and white outfit  holding a broom  with a traditional asian architecture in the background  [5, 16, 0, 23, 4, 18, 304, 38, 0, 1307, 8, 0, 741, 98, 3317, 1, 2, 82]\n",
      "1010470346.jpg\n",
      " two men in florescent vests are standing next to parked cars in front of a small building while one of them converses with a driver and a woman on a bike is seen riding by [10, 27, 1, 5000, 532, 11, 29, 64, 12, 533, 487, 1, 35, 7, 0, 63, 72, 21, 39, 7, 141, 4380, 8, 0, 1097, 4, 0, 9, 3, 0, 92, 6, 759, 74, 41]\n",
      "20 torch.Size([2, 3, 224, 224]) torch.Size([2, 35])\n",
      "1010673430.jpg\n",
      " a little girl in a pink shirt and a little girl in an orange shirt sitting in the grass  [0, 50, 25, 1, 0, 83, 19, 4, 0, 50, 25, 1, 15, 76, 19, 26, 1, 2, 90]\n",
      "101093029.jpg\n",
      " a person in tan pants is inside a silver mobile object while people watch  [0, 54, 1, 260, 125, 6, 254, 0, 612, 1638, 428, 21, 13, 210]\n",
      "21 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "101093045.jpg\n",
      " a man in black approaches a strange silver object containing a person  while many onlookers observe from behind a roped off barrier  [0, 5, 1, 20, 2029, 0, 1527, 612, 428, 2129, 0, 54, 21, 191, 543, 1586, 58, 84, 0, 4937, 110, 1587]\n",
      "1011572216.jpg\n",
      " bride and groom walking side by side out of focus on pathway next to brick building  [634, 4, 970, 37, 143, 41, 143, 69, 7, 2366, 3, 1823, 64, 12, 268, 72]\n",
      "22 torch.Size([2, 3, 224, 224]) torch.Size([2, 22])\n",
      "1012150929.jpg\n",
      " a little boy plays with a nintendo gamecube controller inside a mcdonald s  [0, 50, 28, 130, 8, 0, 4221, 5000, 5000, 254, 0, 2441, 103]\n",
      "1012212859.jpg\n",
      " white dog with brown ears standing near water with head turned to one side  [18, 30, 8, 56, 1114, 29, 77, 40, 8, 149, 1420, 12, 39, 143]\n",
      "23 torch.Size([2, 3, 224, 224]) torch.Size([2, 14])\n",
      "1012328893.jpg\n",
      " a group of people picnicking at picnic tables in front of a playground  [0, 33, 7, 13, 5000, 14, 876, 643, 1, 35, 7, 0, 457]\n",
      "101262930.jpg\n",
      " two asian or spanish people  a woman and a man  sitting together in front of a glass window as cars pass  [10, 98, 209, 2572, 13, 0, 9, 4, 0, 5, 26, 133, 1, 35, 7, 0, 398, 228, 48, 487, 731]\n",
      "24 torch.Size([2, 3, 224, 224]) torch.Size([2, 21])\n",
      "1013536888.jpg\n",
      " women are seated at a picnic table eating  while a man in a white tshirt and a yellow and orange balloon design on his head stands in the background  [43, 11, 413, 14, 0, 876, 87, 193, 21, 0, 5, 1, 0, 18, 183, 4, 0, 53, 4, 76, 645, 1899, 3, 22, 149, 80, 1, 2, 82]\n",
      "101362133.jpg\n",
      " a young female student performing a downward kick to break a board held by her karate instructor  [0, 17, 172, 1315, 177, 0, 3318, 703, 12, 720, 0, 359, 742, 41, 36, 743, 2367]\n",
      "25 torch.Size([2, 3, 224, 224]) torch.Size([2, 29])\n",
      "101362650.jpg\n",
      " two people are demonstrating martial arts to a crowd and jumping over three youngsters who are crouched on the mat  [10, 13, 11, 2104, 507, 555, 12, 0, 79, 4, 86, 66, 42, 3848, 168, 11, 1773, 3, 2, 985]\n",
      "1014609273.jpg\n",
      " people on two balconies and a man climbing up a pipe towards the lower balcony getting liquid poured on him  [13, 3, 10, 4222, 4, 0, 5, 240, 45, 0, 1047, 272, 2, 2632, 964, 241, 1431, 5000, 3, 121]\n",
      "26 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "101471792.jpg\n",
      " a man with a red jacket is shielding himself from the sun trying to read a piece of paper  [0, 5, 8, 0, 24, 73, 6, 3609, 836, 58, 2, 460, 257, 12, 1468, 0, 303, 7, 325]\n",
      "1014785440.jpg\n",
      " two men are carrying children on their backs down the street on a sunny day in a nice neighborhood  [10, 27, 11, 134, 55, 3, 59, 1279, 34, 2, 32, 3, 0, 378, 178, 1, 0, 960, 1366]\n",
      "27 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "1015118661.jpg\n",
      " smiling boy in white shirt and blue jeans in front of rock wall with man in overalls behind him  [127, 28, 1, 18, 19, 4, 23, 129, 1, 35, 7, 162, 102, 8, 5, 1, 1000, 84, 121]\n",
      "1015584366.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a mottled black and gray dog in a blue collar jumping over a fallen tree  [0, 5000, 20, 4, 116, 30, 1, 0, 23, 735, 86, 66, 0, 883, 158]\n",
      "28 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "101559400.jpg\n",
      " men in the business suits are crossing the street  and there are people with placards are gathering on the street  [27, 1, 2, 642, 525, 11, 458, 2, 32, 4, 146, 11, 13, 8, 5000, 11, 663, 3, 2, 32]\n",
      "1015712668.jpg\n",
      " a man in a red longsleeved shirt bikes over a body of water on a bridge  [0, 5, 1, 0, 24, 1088, 19, 450, 66, 0, 295, 7, 40, 3, 0, 346]\n",
      "29 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "10160966.jpg\n",
      " a barefooted man wearing olive green shorts grilling hotdogs on a small propane grill while holding a blue plastic cup  [0, 2473, 5, 16, 2633, 44, 118, 1291, 2368, 3, 0, 63, 5000, 592, 21, 38, 0, 23, 407, 524]\n",
      "101654506.jpg\n",
      " the white and brown dog is running over the surface of the snow  [2, 18, 4, 56, 30, 6, 71, 66, 2, 990, 7, 2, 91]\n",
      "30 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "1016626169.jpg\n",
      " man on scooter attracts attention from some of those in large crowd  [5, 3, 699, 5000, 1368, 58, 67, 7, 2886, 1, 51, 79]\n",
      "101669240.jpg\n",
      " a man in a hat is displaying pictures next to a skier in a blue hat  [0, 5, 1, 0, 60, 6, 1382, 443, 64, 12, 0, 619, 1, 0, 23, 60]\n",
      "31 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "1016887272.jpg\n",
      " several climbers in a row are climbing the rock while the man in red watches and holds the line  [115, 2507, 1, 0, 824, 11, 240, 2, 162, 21, 2, 5, 1, 24, 216, 4, 136, 2, 263]\n",
      "1017675163.jpg\n",
      " a woman wearing an orange leotard doing gymnastics in front of an audience  [0, 9, 16, 15, 76, 1950, 157, 1694, 1, 35, 7, 15, 454]\n",
      "32 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "1018057225.jpg\n",
      " a boy in a black tshirt and blue jeans is pushing a toy three wheeler around a small pool  [0, 28, 1, 0, 20, 183, 4, 23, 129, 6, 432, 0, 215, 42, 2634, 75, 0, 63, 159]\n",
      "1018148011.jpg\n",
      " a group of people stand in the back of a truck filled with cotton  [0, 33, 7, 13, 100, 1, 2, 155, 7, 0, 282, 482, 8, 2154]\n",
      "33 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "101859883.jpg\n",
      " a woman with a red jacket and headscarf looks out over a scenic view of a bay through a set of pay binoculars on a viewing deck  [0, 9, 8, 0, 24, 73, 4, 1477, 94, 69, 66, 0, 1749, 433, 7, 0, 2302, 57, 0, 355, 7, 2105, 1863, 3, 0, 1774, 851]\n",
      "10188041.jpg\n",
      " a man in a black coat walks past a red spaceship with a parking ticket stuck to its window  [0, 5, 1, 0, 20, 187, 124, 247, 0, 24, 5000, 8, 0, 572, 2509, 2241, 12, 207, 228]\n",
      "34 torch.Size([2, 3, 224, 224]) torch.Size([2, 27])\n",
      "1019077836.jpg\n",
      " large brown dog running away from the sprinkler in the grass  [51, 56, 30, 71, 334, 58, 2, 1613, 1, 2, 90]\n",
      "101958970.jpg\n",
      " a man and a little girl happily posing in front of their cart in a supermarket  [0, 5, 4, 0, 50, 25, 1216, 253, 1, 35, 7, 59, 289, 1, 0, 2271]\n",
      "35 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "1019604187.jpg\n",
      " a dog prepares to catch a thrown object in a field with nearby cars  [0, 30, 548, 12, 364, 0, 1099, 428, 1, 0, 78, 8, 441, 487]\n",
      "102030371.jpg\n",
      " a man wearing a green shirt and a dark colored vest is sitting at a table with a glass of water in front of him  [0, 5, 16, 0, 44, 19, 4, 0, 173, 306, 298, 6, 26, 14, 0, 87, 8, 0, 398, 7, 40, 1, 35, 7, 121]\n",
      "36 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "1020651753.jpg\n",
      " a white dog is trying to catch a ball in midair over a grassy field  [0, 18, 30, 6, 257, 12, 364, 0, 62, 1, 415, 66, 0, 284, 78]\n",
      "1021293940.jpg\n",
      " two men are hiking in a forest where snow is partially covering the ground  [10, 27, 11, 707, 1, 0, 417, 446, 91, 6, 1668, 910, 2, 169]\n",
      "37 torch.Size([2, 3, 224, 224]) torch.Size([2, 15])\n",
      "1021332107.jpg\n",
      " an elderly man with facial hair and glasses  standing in his living room holding a hutch cabinet with glass doors  [15, 233, 5, 8, 2242, 106, 4, 147, 29, 1, 22, 1012, 188, 38, 0, 5000, 3410, 8, 398, 1501]\n",
      "1021439420.jpg\n",
      " two guys sitting on the floor  with the guy in the green jacket reading a piece of paper  [10, 320, 26, 3, 2, 217, 8, 2, 161, 1, 2, 44, 73, 206, 0, 303, 7, 325]\n",
      "38 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "1021442086.jpg\n",
      " a bearded man wearing glasses  a red shirt  and a hat is reading a map while sitting in a car  [0, 620, 5, 16, 147, 0, 24, 19, 4, 0, 60, 6, 206, 0, 1345, 21, 26, 1, 0, 138]\n",
      "1022454332.jpg\n",
      " a large lake with a lone duck swimming in it with several people around the edge of it  [0, 51, 277, 8, 0, 904, 1695, 245, 1, 114, 8, 115, 13, 75, 2, 429, 7, 114]\n",
      "39 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "1022454428.jpg\n",
      " a couple and an infant  being held by the male  sitting next to a pond with a nearby stroller  [0, 165, 4, 15, 979, 174, 742, 41, 2, 164, 26, 64, 12, 0, 826, 8, 0, 441, 654]\n",
      "102268204.jpg\n",
      " three men are standing at night near a black car near a shelter filled with bicycles  [42, 27, 11, 29, 14, 309, 77, 0, 20, 138, 77, 0, 2887, 482, 8, 631]\n",
      "40 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "1022975728.jpg\n",
      " a black lab with tags frolicks in the water  [0, 20, 821, 8, 2369, 5000, 1, 2, 40]\n",
      "102351840.jpg\n",
      " a man is drilling through the frozen ice of a pond  [0, 5, 6, 2510, 57, 2, 1931, 266, 7, 0, 826]\n",
      "41 torch.Size([2, 3, 224, 224]) torch.Size([2, 11])\n",
      "1024138940.jpg\n",
      " two different breeds of brown and white dogs play on the beach  [10, 567, 5000, 7, 56, 4, 18, 99, 131, 3, 2, 81]\n",
      "102455176.jpg\n",
      " an ice climber in a blue jacket and black pants is scaling a frozen ice wall  [15, 266, 986, 1, 0, 23, 73, 4, 20, 125, 6, 2303, 0, 1931, 266, 102]\n",
      "42 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "1024613706.jpg\n",
      " two young men and a young lady walk through a field near the water  [10, 17, 27, 4, 0, 17, 119, 142, 57, 0, 78, 77, 2, 40]\n",
      "102520526.jpg\n",
      " a man in black attire shovels snow into the street  disregarding all public safety  [0, 5, 1, 20, 605, 1575, 91, 65, 2, 32, 5000, 220, 393, 490]\n",
      "43 torch.Size([2, 3, 224, 224]) torch.Size([2, 14])\n",
      "102575576.jpg\n",
      " a couple in their wedding attire stand behind a table with a wedding cake and flowers  [0, 165, 1, 59, 575, 605, 100, 84, 0, 87, 8, 0, 575, 679, 4, 363]\n",
      "102617084.jpg\n",
      " five snowmobile riders all wearing helmets and goggles line up in a snowy clearing in a forest in front of their snowmobiles  they are all wearing black snow pants and from left to right they are wearing a black coat  white coat  red coat  blue coat  and black coat  [262, 3502, 1059, 220, 16, 664, 4, 536, 263, 45, 1, 0, 317, 2155, 1, 0, 417, 1, 35, 7, 59, 5000, 248, 11, 220, 16, 20, 91, 125, 4, 58, 370, 12, 337, 248, 11, 16, 0, 20, 187, 18, 187, 24, 187, 23, 187, 4, 20, 187]\n",
      "44 torch.Size([2, 3, 224, 224]) torch.Size([2, 49])\n",
      "1026685415.jpg\n",
      " a black dog carries a green toy in his mouth as he walks through the grass  [0, 20, 30, 504, 0, 44, 215, 1, 22, 181, 48, 171, 124, 57, 2, 90]\n",
      "1026792563.jpg\n",
      " a group of woman and children are standing or sitting in front of rows of vegetables  [0, 33, 7, 9, 4, 55, 11, 29, 209, 26, 1, 35, 7, 1669, 7, 659]\n",
      "45 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "1027149103.jpg\n",
      " a band is playing in front of an audience and the singer is wearing an orange shirt and has tattoos on his arm  [0, 198, 6, 31, 1, 35, 7, 15, 454, 4, 2, 844, 6, 16, 15, 76, 19, 4, 108, 1193, 3, 22, 338]\n",
      "1027211271.jpg\n",
      " a young woman with short blondhair wearing jeans and a striped longsleeved sweater jumping in midair on a skateboard with trees in the background  [0, 17, 9, 8, 445, 508, 16, 129, 4, 0, 180, 1088, 274, 86, 1, 415, 3, 0, 258, 8, 244, 1, 2, 82]\n",
      "46 torch.Size([2, 3, 224, 224]) torch.Size([2, 24])\n",
      "1028205764.jpg\n",
      " a man and a little boy in blue life jackets are rowing a yellow canoe  [0, 5, 4, 0, 50, 28, 1, 23, 624, 518, 11, 1396, 0, 53, 862]\n",
      "102851549.jpg\n",
      " a little boy looks at the camera while a woman behind him seems to be laughing very hard and the woman on the right has a big smile on her face  [0, 50, 28, 94, 14, 2, 96, 21, 0, 9, 84, 121, 1006, 12, 212, 389, 267, 326, 4, 2, 9, 3, 2, 337, 108, 0, 269, 502, 3, 36, 145]\n",
      "47 torch.Size([2, 3, 224, 224]) torch.Size([2, 31])\n",
      "102860573.jpg\n",
      " a construction site on a street with three men working  [0, 184, 657, 3, 0, 32, 8, 42, 27, 109]\n",
      "10287332.jpg\n",
      " two men sitting on the roof of a house while another one stands on a ladder  [10, 27, 26, 3, 2, 510, 7, 0, 332, 21, 70, 39, 80, 3, 0, 566]\n",
      "48 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "1028982826.jpg\n",
      " two men  one younger guy with a yellow shirt that has luggage and the other and older guy  are sitting on a bench in front of a large building that has a billboard for glasses and sunglasses  [10, 27, 39, 586, 161, 8, 0, 53, 19, 93, 108, 1015, 4, 2, 61, 4, 105, 161, 11, 26, 3, 0, 140, 1, 35, 7, 0, 51, 72, 93, 108, 0, 1640, 49, 147, 4, 192]\n",
      "1029450589.jpg\n",
      " an adult wearing a gray shirt with red sleeves sleeping on a couch  [15, 405, 16, 0, 116, 19, 8, 24, 2156, 330, 3, 0, 408]\n",
      "49 torch.Size([2, 3, 224, 224]) torch.Size([2, 37])\n",
      "1029737941.jpg\n",
      " boy in brown shirt with headphones on sits on woman s shoulders in a crowd  [28, 1, 56, 19, 8, 729, 3, 89, 3, 9, 103, 767, 1, 0, 79]\n",
      "1029802110.jpg\n",
      " during a gay pride parade in an asian city  some people hold up rainbow flags to show support  [226, 0, 3731, 3248, 470, 1, 15, 98, 101, 67, 13, 514, 45, 1394, 562, 12, 568, 2212]\n",
      "50 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "102998070.jpg\n",
      " a young man in a blue sweater cleaning fish while three other men watch  one a man a cigarette  or match in his month  [0, 17, 5, 1, 0, 23, 274, 479, 462, 21, 42, 61, 27, 210, 39, 0, 5, 0, 455, 209, 606, 1, 22, 5000]\n",
      "1030041880.jpg\n",
      " woman in a green dress with a white top and redhair singing in a room with a blue roof while band equipment is being prepared  [9, 1, 0, 44, 112, 8, 0, 18, 95, 4, 856, 256, 1, 0, 188, 8, 0, 23, 510, 21, 198, 416, 6, 174, 2474]\n",
      "51 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "103031977.jpg\n",
      " four collegeaged individuals  two males and two females  sitting on the concrete steps outside of a brick building [107, 5000, 852, 10, 704, 4, 10, 926, 26, 3, 2, 410, 362, 52, 7, 0, 268, 72]\n",
      "1030985833.jpg\n",
      " the chocolate lab jumps too late to get the toy as the black lab captures it in the driveway  [2, 1842, 821, 179, 2180, 2678, 12, 449, 2, 215, 48, 2, 20, 821, 4225, 114, 1, 2, 2182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "103106960.jpg\n",
      " a young male kneeling in front of a hockey goal with a hockey stick in his right hand  [0, 17, 164, 632, 1, 35, 7, 0, 361, 748, 8, 0, 361, 293, 1, 22, 337, 128]\n",
      "103195344.jpg\n",
      " the man with the backpack is sitting in a buildings courtyard in front of an art sculpture reading  [2, 5, 8, 2, 340, 6, 26, 1, 0, 452, 1316, 1, 35, 7, 15, 495, 798, 206]\n",
      "53 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "1031973097.jpg\n",
      " a toddler in a blue shirt with red shorts and hat looks out from behind a fenced in area of a brick patio  [0, 328, 1, 0, 23, 19, 8, 24, 118, 4, 60, 94, 69, 58, 84, 0, 1383, 1, 167, 7, 0, 268, 1292]\n",
      "103205630.jpg\n",
      " two men  standing on an ice  looking into something covered with a blue tarp  [10, 27, 29, 3, 15, 266, 47, 65, 113, 255, 8, 0, 23, 2305]\n",
      "54 torch.Size([2, 3, 224, 224]) torch.Size([2, 23])\n",
      "1032122270.jpg\n",
      " three dogs are standing in the grass and a person is sitting next to them [42, 99, 11, 29, 1, 2, 90, 4, 0, 54, 6, 26, 64, 12, 141]\n",
      "1032460886.jpg\n",
      " there is a skyscraper in the distance with a man walking in front of the camera  [146, 6, 0, 4383, 1, 2, 442, 8, 0, 5, 37, 1, 35, 7, 2, 96]\n",
      "55 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "103306033.jpg\n",
      " a woman wearing a brown dress with a matching turban is pushing a toddler through a park in his stroller  [0, 9, 16, 0, 56, 112, 8, 0, 865, 1951, 6, 432, 0, 328, 57, 0, 120, 1, 22, 654]\n",
      "103328091.jpg\n",
      " a asian male wearing a red shirt is sitting next to a grocery stand selling fruits  [0, 98, 164, 16, 0, 24, 19, 6, 26, 64, 12, 0, 809, 100, 409, 1435]\n",
      "56 torch.Size([2, 3, 224, 224]) torch.Size([2, 20])\n",
      "1033767085.jpg\n",
      " a young child with a bandaid on his head sits eating at a small white table with a little girl eating off a yellow plate  [0, 17, 46, 8, 0, 5000, 3, 22, 149, 89, 193, 14, 0, 63, 18, 87, 8, 0, 50, 25, 193, 110, 0, 53, 732]\n",
      "1034276567.jpg\n",
      " a small boy putting something in his mouth with both hands  [0, 63, 28, 426, 113, 1, 22, 181, 8, 349, 154]\n",
      "57 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "1034985636.jpg\n",
      " a man with wild hair rocks a show playing a guitar center stage  [0, 5, 8, 2063, 106, 345, 0, 568, 31, 0, 122, 877, 148]\n",
      "1035019794.jpg\n",
      " a person in a green outfit and helmet is swinging a baseball bat at a tennis ball in front of a fence  [0, 54, 1, 0, 44, 304, 4, 205, 6, 561, 0, 166, 829, 14, 0, 208, 62, 1, 35, 7, 0, 259]\n",
      "58 torch.Size([2, 3, 224, 224]) torch.Size([2, 22])\n",
      "10350842.jpg\n",
      " a man in a green shirt and red life jacket is sitting in a canoe drifting around the lake  [0, 5, 1, 0, 44, 19, 4, 24, 624, 73, 6, 26, 1, 0, 862, 5000, 75, 2, 277]\n",
      "1035392784.jpg\n",
      " a man in black with long hair playing a guitar with stickers on it  [0, 5, 1, 20, 8, 186, 106, 31, 0, 122, 8, 5000, 3, 114]\n",
      "59 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "103625356.jpg\n",
      " a man in a black shirt crouches next to a dirty unfinished concrete edge while several shovels and other tools lean against a tree nearby  [0, 5, 1, 0, 20, 19, 1434, 64, 12, 0, 778, 2940, 410, 429, 21, 115, 1575, 4, 61, 1226, 1932, 221, 0, 158, 441]\n",
      "103631543.jpg\n",
      " a caucasian young woman in a white hooded sweatshirt sits at a table set for two in a lowly lit restaurant  [0, 1209, 17, 9, 1, 0, 18, 938, 414, 89, 14, 0, 87, 355, 49, 10, 1, 0, 5000, 676, 249]\n",
      "60 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "1039360778.jpg\n",
      " a woman in a blue shirt and sunglasses lies on a red blanket in a grassy field with a bike set down next to her  [0, 9, 1, 0, 23, 19, 4, 192, 1071, 3, 0, 24, 589, 1, 0, 284, 78, 8, 0, 92, 355, 34, 64, 12, 36]\n",
      "103953336.jpg\n",
      " someone is standing on a long bridge with a cloudy blue sky in the background  [273, 6, 29, 3, 0, 186, 346, 8, 0, 1129, 23, 437, 1, 2, 82]\n",
      "61 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "1039637574.jpg\n",
      " a man in light colored clothing photographs a group of men wearing dark suits and hats standing around a woman dressed in a strapless gown  [0, 5, 1, 265, 306, 236, 1478, 0, 33, 7, 27, 16, 173, 525, 4, 270, 29, 75, 0, 9, 68, 1, 0, 5000, 1614]\n",
      "10404007.jpg\n",
      " young white boy on a grassy area with a sad expression on his face  [17, 18, 28, 3, 0, 284, 167, 8, 0, 2107, 1933, 3, 22, 145]\n",
      "62 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "1040426962.jpg\n",
      " a man is cooking what appears to be green beans in a grill basket on a grill with a yellow lab standing by him while someone takes a picture from above  [0, 5, 6, 420, 366, 403, 12, 212, 44, 3733, 1, 0, 592, 597, 3, 0, 592, 8, 0, 53, 821, 29, 41, 121, 21, 273, 333, 0, 123, 58, 312]\n",
      "104136873.jpg\n",
      " three people are on a hilltop overlooking a green valley  [42, 13, 11, 3, 0, 5000, 614, 0, 44, 1952]\n",
      "63 torch.Size([2, 3, 224, 224]) torch.Size([2, 31])\n",
      "104180524.jpg\n",
      " a group of tourists is crossing a bridge that connects a walking path to a trail of nature  [0, 33, 7, 1156, 6, 458, 0, 346, 93, 5000, 0, 37, 287, 12, 0, 473, 7, 1801]\n",
      "1042020065.jpg\n",
      " a young boy wearing a jersey looks down over a ledge at many boats in the water  [0, 17, 28, 16, 0, 453, 94, 34, 66, 0, 693, 14, 191, 911, 1, 2, 40]\n",
      "64 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "1042359076.jpg\n",
      " a older man holds an object and is looking at two young girls that sit across from him at a table that is outside  [0, 105, 5, 136, 15, 428, 4, 6, 47, 14, 10, 17, 88, 93, 152, 190, 58, 121, 14, 0, 87, 93, 6, 52]\n",
      "1042590306.jpg\n",
      " a man is holding the hand of a woman up to his mouth in front of some buildings while another man looks on  [0, 5, 6, 38, 2, 128, 7, 0, 9, 45, 12, 22, 181, 1, 35, 7, 67, 452, 21, 70, 5, 94, 3]\n",
      "65 torch.Size([2, 3, 224, 224]) torch.Size([2, 24])\n",
      "104285082.jpg\n",
      " a man with glasses is sitting a chair playing the oboe while a man in a purple shirt plays percussion and spectators look on  [0, 5, 8, 147, 6, 26, 0, 231, 31, 2, 5000, 21, 0, 5, 1, 0, 189, 19, 130, 4385, 4, 545, 182, 3]\n",
      "1043819504.jpg\n",
      " a group of 11 people in winter wear such as beanies  skiing jackets  gloves and backpacks are standing in snow paddles outside a house made of ice blocks while a person in front of the door seems to be leading them  [0, 33, 7, 3612, 13, 1, 461, 927, 2941, 48, 5000, 723, 518, 500, 4, 1194, 11, 29, 1, 91, 1696, 52, 0, 332, 746, 7, 266, 1150, 21, 0, 54, 1, 35, 7, 2, 459, 1006, 12, 212, 1285, 141]\n",
      "66 torch.Size([2, 3, 224, 224]) torch.Size([2, 41])\n",
      "1043910339.jpg\n",
      " a baseball player in red leaps into the air to avoid a player in blue  who is shortly behind him as he catches the ball  while other blue team members look on in the background  [0, 166, 104, 1, 24, 618, 65, 2, 97, 12, 2890, 0, 104, 1, 23, 168, 6, 5000, 84, 121, 48, 171, 863, 2, 62, 21, 61, 23, 238, 764, 182, 3, 1, 2, 82]\n",
      "1044434135.jpg\n",
      " a man in the foreground wearing a white shirt and black pants riding a bicycle with a basket in a busy street  [0, 5, 1, 2, 607, 16, 0, 18, 19, 4, 20, 125, 74, 0, 153, 8, 0, 597, 1, 0, 235, 32]\n",
      "67 torch.Size([2, 3, 224, 224]) torch.Size([2, 35])\n",
      "1044764620.jpg\n",
      " a man on a ladder is painting a brick wall in a warehouse  [0, 5, 3, 0, 566, 6, 313, 0, 268, 102, 1, 0, 2181]\n",
      "1044798682.jpg\n",
      " a man wearing a hat and a white shirt is cleaning windows  [0, 5, 16, 0, 60, 4, 0, 18, 19, 6, 479, 843]\n",
      "68 torch.Size([2, 3, 224, 224]) torch.Size([2, 13])\n",
      "1045124251.jpg\n",
      " a blondhaired man with glasses and a blank tank top is sitting next to another man with glasses and headphones while sitting on a train  [0, 888, 5, 8, 147, 4, 0, 4736, 307, 95, 6, 26, 64, 12, 70, 5, 8, 147, 4, 729, 21, 26, 3, 0, 224]\n",
      "1045309098.jpg\n",
      " juggling her shopping bags and betty boop backpack and young woman crosses a city street  [1072, 36, 308, 404, 4, 5000, 5000, 340, 4, 17, 9, 1286, 0, 101, 32]\n",
      "69 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "1045521051.jpg\n",
      " a young man sits on the floor by the television with a fast food meal in front of him  [0, 17, 5, 89, 3, 2, 217, 41, 2, 1728, 8, 0, 1217, 139, 635, 1, 35, 7, 121]\n",
      "10459869.jpg\n",
      " several asian people are eating together around a table  [115, 98, 13, 11, 193, 133, 75, 0, 87]\n",
      "70 torch.Size([2, 3, 224, 224]) torch.Size([2, 19])\n",
      "104669470.jpg\n",
      " a woman with flowers in her hair balances a large bowl of assorted fruits on her head  [0, 9, 8, 363, 1, 36, 106, 1538, 0, 51, 627, 7, 3736, 1435, 3, 36, 149]\n",
      "104700891.jpg\n",
      " a man wearing a gray jacket and blue work pants is standing in a hole in the ground [0, 5, 16, 0, 116, 73, 4, 23, 232, 125, 6, 29, 1, 0, 769, 1, 2, 169]\n",
      "71 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "104763189.jpg\n",
      " a shirtless young boy on a bicycle too big for him being supported by 3 other young shirtless boys in a grass field  [0, 376, 17, 28, 3, 0, 153, 2180, 269, 49, 121, 174, 4094, 41, 590, 61, 17, 376, 126, 1, 0, 90, 78]\n",
      "1047921035.jpg\n",
      " a man in a bandanna and a dirty tshirt  and a woman in a graphic tshirt  are standing next to a oblong hole with smoke coming out  [0, 5, 1, 0, 980, 4, 0, 778, 183, 4, 0, 9, 1, 0, 3415, 183, 11, 29, 64, 12, 0, 5000, 769, 8, 1060, 687, 69]\n",
      "72 torch.Size([2, 3, 224, 224]) torch.Size([2, 27])\n",
      "104816788.jpg\n",
      " this little baby girl dressed whimsically in striped pantaloons is pictured so that one must ask which is bigger  her feet or her sleepy smile  [223, 50, 144, 25, 68, 5000, 1, 180, 5000, 6, 2411, 1358, 93, 39, 4738, 5000, 381, 6, 4386, 36, 554, 209, 36, 4558, 502]\n",
      "104824673.jpg\n",
      " an african individual is looking into the camera while dressed in varying beads and decorations that have a cultural relevance  [15, 379, 1228, 6, 47, 65, 2, 96, 21, 68, 1, 2891, 2034, 4, 2276, 93, 375, 0, 3182, 5000]\n",
      "73 torch.Size([2, 3, 224, 224]) torch.Size([2, 25])\n",
      "104835889.jpg\n",
      " a samurai warrior in full black dress takes his sword from the sheath on an outdoor training mat  [0, 5000, 4387, 1, 324, 20, 112, 333, 22, 1398, 58, 2, 5000, 3, 15, 246, 1577, 985]\n",
      "1048710776.jpg\n",
      " a couple of several people sitting on a ledge overlooking the beach [0, 165, 7, 115, 13, 26, 3, 0, 693, 614, 2, 81]\n",
      "74 torch.Size([2, 3, 224, 224]) torch.Size([2, 18])\n",
      "1049955899.jpg\n",
      " a group of men and women are hiking through a dried up river bed littered with large rocks  which runs between two rocky walls in a dense green forest  [0, 33, 7, 27, 4, 43, 11, 707, 57, 0, 2892, 45, 301, 440, 3737, 8, 51, 345, 381, 197, 385, 10, 476, 1027, 1, 0, 5000, 44, 417]\n",
      "105077209.jpg\n",
      " a man with a badge is standing on a wood floor next to a structure  [0, 5, 8, 0, 4560, 6, 29, 3, 0, 401, 217, 64, 12, 0, 505]\n",
      "75 torch.Size([2, 3, 224, 224]) torch.Size([2, 29])\n",
      "1051205546.jpg\n",
      " two women in colorful dresses are sweeping water out of the street while a man watches  [10, 43, 1, 250, 546, 11, 853, 40, 69, 7, 2, 32, 21, 0, 5, 216]\n",
      "1051290485.jpg\n",
      " two separate women  one wearing a white top and black pants  the other wearing all black and a pair of pink flipflops  kneel on a gray brick sidewalk path in front of green and red bushes  [10, 2679, 43, 39, 16, 0, 18, 95, 4, 20, 125, 2, 61, 16, 220, 20, 4, 0, 905, 7, 83, 1503, 3615, 3, 0, 116, 268, 85, 287, 1, 35, 7, 44, 4, 24, 1419]\n",
      "76 torch.Size([2, 3, 224, 224]) torch.Size([2, 36])\n",
      "1051953669.jpg\n",
      " a young girl wearing a multicolored holding an orange ball in her right hand walking through bright green grass behind a house [0, 17, 25, 16, 0, 796, 38, 15, 76, 62, 1, 36, 337, 128, 37, 57, 281, 44, 90, 84, 0, 332]\n",
      "105223874.jpg\n",
      " a young child wearing a blue shirt and sandals playing with a red toy while sitting on the floor  [0, 17, 46, 16, 0, 23, 19, 4, 739, 31, 8, 0, 24, 215, 21, 26, 3, 2, 217]\n",
      "77 torch.Size([2, 3, 224, 224]) torch.Size([2, 22])\n",
      "1052358063.jpg\n",
      " a little boy skateboarder is doing a trick on his board while another young skateboarder watches  [0, 50, 28, 431, 6, 157, 0, 311, 3, 22, 359, 21, 70, 17, 431, 216]\n",
      "1053116826.jpg\n",
      " two little children  one boy and one girl  laugh as they sit on the grass  [10, 50, 55, 39, 28, 4, 39, 25, 1250, 48, 248, 152, 3, 2, 90]\n",
      "78 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "1053243591.jpg\n",
      " young woman holding a bottle  reaching out from a bench to shake a man s hand  [17, 9, 38, 0, 512, 820, 69, 58, 0, 140, 12, 3249, 0, 5, 103, 128]\n",
      "105335409.jpg\n",
      " man talking on a microphone and a crowd of people in front of him  [5, 117, 3, 0, 204, 4, 0, 79, 7, 13, 1, 35, 7, 121]\n",
      "79 torch.Size([2, 3, 224, 224]) torch.Size([2, 16])\n",
      "105342180.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a girl paddling down a large river  as seen from behind her  [0, 25, 1219, 34, 0, 51, 301, 48, 759, 58, 84, 36]\n",
      "1053804096.jpg\n",
      " a girl with pigtails is playing in the ocean by the beach  [0, 25, 8, 1822, 6, 31, 1, 2, 225, 41, 2, 81]\n",
      "80 torch.Size([2, 3, 224, 224]) torch.Size([2, 12])\n",
      "1054620089.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0ee7edd4ab35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-3d912cd29b8f>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_encoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1802\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(data_loader):\n",
    "    print(idx, data[0].shape, data[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "    \n",
    "        self.model = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        \n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, features_dim, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(features_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, features, hidden):\n",
    "        features = self.linear1(features)\n",
    "        hidden = hidden[0]\n",
    "        \n",
    "        hidden = self.linear2(hidden)\n",
    "        \n",
    "        score = torch.nn.functional.tanh(hidden + features)\n",
    "        \n",
    "        attention_weights = torch.nn.functional.softmax(self.linear3(score), dim=1)\n",
    "        \n",
    "        attention_vectors = attention_weights * score\n",
    "        attention_vectors = torch.mean(attention_vectors, dim=1)\n",
    "        \n",
    "        return attention_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, embedding_dim, num_layers, vocab_len):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_dim + embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_len)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.linear(x)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, loader):\n",
    "    for idx, data in enumerate(loader):\n",
    "        inputs = data[0].cuda()\n",
    "        enc_embeds = encoder(inputs)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.1206,  0.0362, -0.2240,  0.2395, -0.0155, -0.0090,  0.0532,\n",
      "           0.1678,  0.1748,  0.0265]]], grad_fn=<ViewBackward>), tensor([[[-0.4015,  0.0666, -0.2668,  0.4081, -0.0563, -0.0465,  0.1942,\n",
      "           0.5321,  1.1125,  0.0434]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1229, -0.2185, -0.4701,  0.4513, -0.1097, -0.0544,  0.0641,\n",
      "           0.0982,  0.2417, -0.0161]]], grad_fn=<ViewBackward>), tensor([[[-0.3627, -0.7442, -0.5807,  0.9857, -0.2122, -0.3104,  0.3643,\n",
      "           0.7520,  0.9708, -0.0301]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1978, -0.4159, -0.3750,  0.3909, -0.0475, -0.1323,  0.1368,\n",
      "           0.1838,  0.1406,  0.0098]]], grad_fn=<ViewBackward>), tensor([[[-0.4194, -1.0675, -0.6118,  1.0504, -0.1487, -0.6193,  0.3856,\n",
      "           0.7525,  0.7458,  0.0186]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1297, -0.4165, -0.4629,  0.4339, -0.0909, -0.0684,  0.1993,\n",
      "           0.1639,  0.0761, -0.0210]]], grad_fn=<ViewBackward>), tensor([[[-0.3372, -1.0228, -0.7809,  0.8578, -0.2588, -0.4401,  0.6148,\n",
      "           0.7217,  0.2732, -0.0438]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.2057, -0.2723, -0.6325,  0.2759, -0.1528, -0.0230,  0.2012,\n",
      "           0.2058,  0.1101, -0.0336]]], grad_fn=<ViewBackward>), tensor([[[-0.4886, -1.1653, -0.9595,  0.8851, -0.3825, -0.1090,  0.6695,\n",
      "           0.6666,  0.3274, -0.0479]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1616, -0.3718, -0.6399,  0.3816, -0.0998, -0.0266,  0.2639,\n",
      "           0.1106,  0.0792, -0.0279]]], grad_fn=<ViewBackward>), tensor([[[-0.6110, -1.1068, -0.9943,  1.0083, -0.3380, -0.1011,  0.8243,\n",
      "           0.6769,  0.3943, -0.0436]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.2306, -0.2530, -0.4459,  0.3665, -0.1622, -0.0505,  0.2276,\n",
      "           0.2180,  0.0750,  0.0037]]], grad_fn=<ViewBackward>), tensor([[[-0.5373, -1.1525, -0.6722,  1.3338, -0.3317, -0.1731,  0.7921,\n",
      "           0.8031,  0.4711,  0.0061]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.1683, -0.2653, -0.4759,  0.4070, -0.2023, -0.0212,  0.1875,\n",
      "           0.1545,  0.1794, -0.0740]]], grad_fn=<ViewBackward>), tensor([[[-0.4550, -1.2952, -0.6545,  1.3953, -0.5322, -0.0716,  0.6913,\n",
      "           0.6491,  0.4697, -0.1262]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.3001, -0.2225, -0.5200,  0.4889, -0.2593, -0.0178,  0.1262,\n",
      "           0.1460,  0.1495, -0.1430]]], grad_fn=<ViewBackward>), tensor([[[-0.6232, -0.8006, -0.7220,  1.2767, -0.4731, -0.0646,  0.7088,\n",
      "           0.7051,  0.6424, -0.2054]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.2582, -0.2524, -0.4896,  0.2897, -0.3073, -0.0135,  0.2436,\n",
      "           0.1528,  0.1264, -0.1601]]], grad_fn=<ViewBackward>), tensor([[[-0.4844, -1.1005, -0.6415,  1.1602, -0.5638, -0.0622,  0.7492,\n",
      "           0.7165,  0.4102, -0.2923]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 10\n",
    "embedding_dim = 49\n",
    "hidden_dim = 10\n",
    "num_layers = 1\n",
    "vocab_len = 29\n",
    "model = Decoder(hidden_dim, embedding_dim, num_layers, vocab_len)\n",
    "attention = Attention(embedding_dim, hidden_dim)\n",
    "encoder_output = torch.rand(batch_size, 512, 49)\n",
    "#inputs from embeddings [batch_size, seq_len, embedding_dim + hidden_dim]\n",
    "input_str = torch.rand(batch_size, seq_len, embedding_dim)\n",
    "hidden = torch.rand(2, num_layers, batch_size, hidden_dim)\n",
    "\n",
    "for i in range(input_str.shape[1]):\n",
    "    \n",
    "    attention_vector = attention(encoder_output, hidden)\n",
    "    lstm_input = torch.unsqueeze(torch.cat([input_str[:, i, :], attention_vector], dim=-1), 1)\n",
    "    output, hidden = model(lstm_input, hidden)\n",
    "    print(hidden)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 29])\n",
      "(tensor([[[ 0.1567, -0.3082,  0.2041, -0.2354,  0.0321, -0.0569,  0.0033,\n",
      "          -0.0245,  0.1133,  0.1696]]], grad_fn=<ViewBackward>), tensor([[[ 0.4709, -0.5611,  0.3368, -0.3590,  0.0493, -0.1002,  0.0086,\n",
      "          -0.0744,  0.4667,  0.2902]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
